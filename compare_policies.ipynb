{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정책 성능 비교 노트북\n",
    "\n",
    "이 노트북에서는 DRL 모델과 베이스라인 정책들(Primary-Only, NPCA-Only, Random)의 성능을 비교분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 라이브러리 및 모듈 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 라이브러리 import 완료!\n",
      "PyTorch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# 프로젝트 모듈들 import\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from drl_framework.network import DQN\n",
    "from drl_framework.configs import PPDU_DURATION_VARIANTS, PPDU_DURATION, RADIO_TRANSITION_TIME, OBSS_GENERATION_RATE\n",
    "from drl_framework.random_access import STA, Simulator, Channel\n",
    "\n",
    "# 스타일 설정\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ 모든 라이브러리 import 완료!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 베이스라인 정책들 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 베이스라인 정책들 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "class BaselinePolicies:\n",
    "    \"\"\"베이스라인 정책들 모음\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def primary_only(obs_dict):\n",
    "        \"\"\"항상 Primary 채널에서 대기\"\"\"\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod \n",
    "    def npca_only(obs_dict):\n",
    "        \"\"\"항상 NPCA로 스위치\"\"\"\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_policy(obs_dict):\n",
    "        \"\"\"랜덤 액션 선택\"\"\"\n",
    "        return np.random.randint(0, 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def smart_threshold(obs_dict, threshold=30):\n",
    "        \"\"\"임계값 기반 정책 (OBSS > threshold이면 스위치)\"\"\"\n",
    "        obss_remain = obs_dict.get('primary_channel_obss_occupied_remained', 0)\n",
    "        return 1 if obss_remain > threshold else 0\n",
    "\n",
    "print(\"🎯 베이스라인 정책들 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: DRL 모델 로더 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 DRL 모델 로더 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "class DRLModelLoader:\n",
    "    \"\"\"DRL 모델 로드 및 관리 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cpu')\n",
    "        self.loaded_models = {}\n",
    "    \n",
    "    def find_models(self, model_dir='./obss_comparison_results'):\n",
    "        \"\"\"사용 가능한 모델들 찾기\"\"\"\n",
    "        model_dir = Path(model_dir)\n",
    "        model_files = list(model_dir.glob('*/model.pth'))\n",
    "        \n",
    "        models_info = []\n",
    "        for model_file in model_files:\n",
    "            try:\n",
    "                checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)\n",
    "                info = {\n",
    "                    'path': model_file,\n",
    "                    'name': model_file.parent.name,\n",
    "                    'obss_duration': checkpoint.get('obss_duration', 'unknown'),\n",
    "                    'ppdu_variant': checkpoint.get('ppdu_variant', 'unknown'),\n",
    "                    'ppdu_duration': checkpoint.get('ppdu_duration', 'unknown'),\n",
    "                    'steps_done': checkpoint.get('steps_done', 'unknown')\n",
    "                }\n",
    "                models_info.append(info)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 모델 로드 실패: {model_file} - {e}\")\n",
    "        \n",
    "        return models_info\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"특정 모델 로드\"\"\"\n",
    "        model_path = str(model_path)\n",
    "        \n",
    "        if model_path in self.loaded_models:\n",
    "            return self.loaded_models[model_path]\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
    "            \n",
    "            # DQN 모델 초기화 및 가중치 로드\n",
    "            model = DQN(n_observations=4, n_actions=2).to(self.device)\n",
    "            model.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "            model.eval()\n",
    "            \n",
    "            # 모델 래퍼 생성\n",
    "            def drl_policy(obs_dict):\n",
    "                # 관측을 벡터로 변환\n",
    "                obs_vector = [\n",
    "                    obs_dict.get('primary_channel_obss_occupied_remained', 0),\n",
    "                    obs_dict.get('radio_transition_time', 1),\n",
    "                    obs_dict.get('tx_duration', 33),\n",
    "                    obs_dict.get('cw_index', 0)\n",
    "                ]\n",
    "                \n",
    "                # 예측\n",
    "                input_tensor = torch.tensor(obs_vector, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(input_tensor)\n",
    "                    action = q_values.argmax(dim=1).item()\n",
    "                return action\n",
    "            \n",
    "            self.loaded_models[model_path] = {\n",
    "                'model': model,\n",
    "                'policy': drl_policy,\n",
    "                'info': checkpoint\n",
    "            }\n",
    "            \n",
    "            return self.loaded_models[model_path]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 모델 로드 실패: {e}\")\n",
    "            return None\n",
    "\n",
    "# 모델 로더 초기화\n",
    "model_loader = DRLModelLoader()\n",
    "print(\"🤖 DRL 모델 로더 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 사용 가능한 모델들 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 발견된 모델 수: 14\n",
      "\n",
      "📋 사용 가능한 모델들:\n",
      "                        name  obss_duration ppdu_variant ppdu_duration  steps_done\n",
      "1          ppdu_long_obss_50             50         long            50         630\n",
      "2         ppdu_short_obss_50             50        short            20        1658\n",
      "3         ppdu_long_obss_200            200         long            50        3084\n",
      "4      trained_model_obss_50             50      unknown       unknown        5014\n",
      "5   ppdu_extra_long_obss_100            100   extra_long            80        1149\n",
      "6     trained_model_obss_100            100      unknown       unknown        5170\n",
      "7         ppdu_long_obss_100            100         long            50        1488\n",
      "8        ppdu_short_obss_100            100        short            20        3246\n",
      "9   ppdu_extra_long_obss_200            200   extra_long            80        2220\n",
      "10      ppdu_medium_obss_100            100       medium            33        1803\n",
      "11       ppdu_medium_obss_50             50       medium            33        1151\n",
      "12      ppdu_medium_obss_200            200       medium            33        4413\n",
      "13       ppdu_short_obss_200            200        short            20        6159\n",
      "14   ppdu_extra_long_obss_50             50   extra_long            80         566\n"
     ]
    }
   ],
   "source": [
    "# 사용 가능한 모델들 찾기\n",
    "available_models = model_loader.find_models()\n",
    "\n",
    "print(f\"🔍 발견된 모델 수: {len(available_models)}\")\n",
    "print(\"\\n📋 사용 가능한 모델들:\")\n",
    "\n",
    "models_df = pd.DataFrame(available_models)\n",
    "if not models_df.empty:\n",
    "    display_df = models_df[['name', 'obss_duration', 'ppdu_variant', 'ppdu_duration', 'steps_done']].copy()\n",
    "    display_df.index = range(1, len(display_df) + 1)\n",
    "    print(display_df.to_string())\n",
    "else:\n",
    "    print(\"❌ 사용 가능한 모델이 없습니다. 먼저 모델을 학습시켜 주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 성능 테스트 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 성능 테스트 함수 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "def test_policy_performance(policy_func, policy_name, test_episodes=30, \n",
    "                          obss_duration=100, ppdu_duration=33, verbose=False):\n",
    "    \"\"\"\n",
    "    주어진 정책의 성능을 테스트\n",
    "    \n",
    "    Args:\n",
    "        policy_func: 테스트할 정책 함수\n",
    "        policy_name: 정책 이름\n",
    "        test_episodes: 테스트 에피소드 수\n",
    "        obss_duration: OBSS 지속 시간\n",
    "        ppdu_duration: PPDU 지속 시간\n",
    "        verbose: 상세 출력 여부\n",
    "    \n",
    "    Returns:\n",
    "        dict: 성능 결과\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # 채널 설정\n",
    "    channels = [\n",
    "        Channel(channel_id=0, obss_generation_rate=OBSS_GENERATION_RATE['secondary']),  # NPCA channel\n",
    "        Channel(channel_id=1, obss_generation_rate=0.01, obss_duration_range=(obss_duration, obss_duration))  # Primary channel\n",
    "    ]\n",
    "    \n",
    "    # 성능 지표\n",
    "    episode_rewards = []\n",
    "    action_counts = [0, 0]  # [Stay, Switch]\n",
    "    total_throughput = 0\n",
    "    decision_count = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(test_episodes):\n",
    "        # STA 설정\n",
    "        sta = STA(\n",
    "            sta_id=0,\n",
    "            channel_id=1,\n",
    "            primary_channel=channels[1],\n",
    "            npca_channel=channels[0],\n",
    "            npca_enabled=True,\n",
    "            ppdu_duration=ppdu_duration,\n",
    "            radio_transition_time=RADIO_TRANSITION_TIME\n",
    "        )\n",
    "        \n",
    "        # 정책 설정\n",
    "        if hasattr(policy_func, '__call__'):\n",
    "            # 베이스라인 정책\n",
    "            def create_fixed_policy(policy_func):\n",
    "                def fixed_action():\n",
    "                    obs_dict = sta.get_obs()\n",
    "                    action = policy_func(obs_dict)\n",
    "                    \n",
    "                    # 액션 카운트\n",
    "                    action_counts[action] += 1\n",
    "                    \n",
    "                    return action\n",
    "                return fixed_action\n",
    "            sta._fixed_action = create_fixed_policy(policy_func)\n",
    "        \n",
    "        # 시뮬레이션 실행\n",
    "        simulator = Simulator(num_slots:=300, channels=channels, stas=[sta])\n",
    "        simulator.device = device\n",
    "        simulator.run()\n",
    "        \n",
    "        # 결과 수집\n",
    "        episode_reward = sta.episode_reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "        total_throughput += sta.channel_occupancy_time\n",
    "        \n",
    "        if verbose and episode < 3:\n",
    "            print(f\"  Episode {episode}: Reward={episode_reward:.1f}, Throughput={sta.channel_occupancy_time}\")\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    # 결과 계산\n",
    "    avg_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    total_actions = sum(action_counts)\n",
    "    \n",
    "    if total_actions > 0:\n",
    "        action_probs = [count/total_actions for count in action_counts]\n",
    "    else:\n",
    "        action_probs = [0.5, 0.5]\n",
    "    \n",
    "    efficiency = total_throughput / (test_episodes * num_slots)  # 총 슬롯 대비 처리량\n",
    "    \n",
    "    result = {\n",
    "        'policy_name': policy_name,\n",
    "        'avg_reward': avg_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'total_throughput': total_throughput,\n",
    "        'efficiency': efficiency,\n",
    "        'action_distribution': action_probs,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'test_time': test_time,\n",
    "        'obss_duration': obss_duration,\n",
    "        'ppdu_duration': ppdu_duration\n",
    "    }\n",
    "    \n",
    "    if not verbose:\n",
    "        print(f\"  {policy_name:12s}: Avg Reward = {avg_reward:6.1f} ± {std_reward:5.1f}, \"\n",
    "              f\"Efficiency = {efficiency:.4f}, Stay/Switch = {action_probs[0]:.2f}/{action_probs[1]:.2f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"🧪 성능 테스트 함수 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 종합 비교 실험 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 종합 비교 실험 함수 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "def run_comprehensive_comparison(selected_models_idx=None, test_scenarios=None):\n",
    "    \"\"\"\n",
    "    종합적인 정책 비교 실험\n",
    "    \n",
    "    Args:\n",
    "        selected_models_idx: 테스트할 모델 인덱스 리스트 (None이면 모든 모델)\n",
    "        test_scenarios: 테스트 시나리오 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_scenarios is None:\n",
    "        test_scenarios = [\n",
    "            {'obss_duration': 50, 'ppdu_duration': 33, 'name': 'Short OBSS'},\n",
    "            {'obss_duration': 150, 'ppdu_duration': 33, 'name': 'Long OBSS'},\n",
    "            {'obss_duration': 200, 'ppdu_duration': 20, 'name': 'Extreme OBSS, Short PPDU'},\n",
    "            {'obss_duration': 200, 'ppdu_duration': 80, 'name': 'Extreme OBSS, Long PPDU'},\n",
    "        ]\n",
    "    \n",
    "    print(\"🚀 종합 비교 실험 시작...\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for scenario_idx, scenario in enumerate(test_scenarios):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"시나리오 {scenario_idx + 1}: {scenario['name']}\")\n",
    "        print(f\"OBSS Duration: {scenario['obss_duration']}, PPDU Duration: {scenario['ppdu_duration']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        scenario_results = []\n",
    "        \n",
    "        # 베이스라인 정책들 테스트\n",
    "        baseline_policies = [\n",
    "            (BaselinePolicies.primary_only, 'Primary-Only'),\n",
    "            (BaselinePolicies.npca_only, 'NPCA-Only'),\n",
    "            (BaselinePolicies.random_policy, 'Random'),\n",
    "            (lambda obs: BaselinePolicies.smart_threshold(obs, 30), 'Smart-30'),\n",
    "            (lambda obs: BaselinePolicies.smart_threshold(obs, 100), 'Smart-100')\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n📊 베이스라인 정책들:\")\n",
    "        for policy_func, policy_name in baseline_policies:\n",
    "            result = test_policy_performance(\n",
    "                policy_func, policy_name,\n",
    "                test_episodes=200,\n",
    "                obss_duration=scenario['obss_duration'],\n",
    "                ppdu_duration=scenario['ppdu_duration']\n",
    "            )\n",
    "            result['scenario'] = scenario['name']\n",
    "            result['policy_type'] = 'Baseline'\n",
    "            scenario_results.append(result)\n",
    "        \n",
    "        # DRL 모델들 테스트\n",
    "        print(\"\\n🤖 DRL 모델들:\")\n",
    "        \n",
    "        models_to_test = available_models\n",
    "        if selected_models_idx is not None:\n",
    "            models_to_test = [available_models[i] for i in selected_models_idx if i < len(available_models)]\n",
    "        \n",
    "        for model_info in models_to_test[:5]:  # 최대 5개 모델만 테스트\n",
    "            loaded_model = model_loader.load_model(model_info['path'])\n",
    "            if loaded_model:\n",
    "                model_name = f\"DRL-{model_info['ppdu_variant']}\"\n",
    "                result = test_policy_performance(\n",
    "                    loaded_model['policy'], model_name,\n",
    "                    test_episodes=200,\n",
    "                    obss_duration=scenario['obss_duration'],\n",
    "                    ppdu_duration=scenario['ppdu_duration']\n",
    "                )\n",
    "                result['scenario'] = scenario['name']\n",
    "                result['policy_type'] = 'DRL'\n",
    "                result['model_info'] = model_info\n",
    "                scenario_results.append(result)\n",
    "        \n",
    "        all_results.extend(scenario_results)\n",
    "        \n",
    "        # 시나리오별 최고 성능 출력\n",
    "        best_result = max(scenario_results, key=lambda x: x['avg_reward'])\n",
    "        print(f\"\\n🏆 최고 성능: {best_result['policy_name']} (평균 보상: {best_result['avg_reward']:.1f})\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"🎯 종합 비교 실험 함수 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 실험 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  실험 시작 시간: 02:12:58\n",
      "🚀 종합 비교 실험 시작...\n",
      "\n",
      "============================================================\n",
      "시나리오 1: Extreme OBSS (200)\n",
      "OBSS Duration: 200, PPDU Duration: 33\n",
      "============================================================\n",
      "\n",
      "📊 베이스라인 정책들:\n",
      "  Primary-Only: Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  NPCA-Only   : Avg Reward =    2.3 ±  23.0, Efficiency = 0.0077, Stay/Switch = 0.00/1.00\n",
      "  Random      : Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  Smart-30    : Avg Reward =  195.5 ±  23.8, Efficiency = 0.6518, Stay/Switch = 0.00/1.00\n",
      "  Smart-100   : Avg Reward =  195.9 ±  23.8, Efficiency = 0.6529, Stay/Switch = 0.00/1.00\n",
      "\n",
      "🤖 DRL 모델들:\n",
      "  DRL-long    : Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.00/1.00\n",
      "  DRL-short   : Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  DRL-long    : Avg Reward =   55.8 ±  22.5, Efficiency = 0.1859, Stay/Switch = 1.00/0.00\n",
      "  DRL-unknown : Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  DRL-extra_long: Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "\n",
      "🏆 최고 성능: Smart-100 (평균 보상: 195.9)\n",
      "\n",
      "============================================================\n",
      "시나리오 2: Short OBSS (50)\n",
      "OBSS Duration: 50, PPDU Duration: 33\n",
      "============================================================\n",
      "\n",
      "📊 베이스라인 정책들:\n",
      "  Primary-Only: Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  NPCA-Only   : Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.00/1.00\n",
      "  Random      : Avg Reward =    1.0 ±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  Smart-30    : Avg Reward =    1.0 ±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  Smart-100   : Avg Reward =    1.3 ±  13.5, Efficiency = 0.0044, Stay/Switch = 1.00/0.00\n",
      "\n",
      "🤖 DRL 모델들:\n",
      "  DRL-long    : Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  DRL-short   : Avg Reward =    1.0 ±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  DRL-long    : Avg Reward =    3.1 ±  25.7, Efficiency = 0.0104, Stay/Switch = 1.00/0.00\n",
      "  DRL-unknown : Avg Reward =    1.0 ±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  DRL-extra_long: Avg Reward =    1.2 ±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "\n",
      "🏆 최고 성능: DRL-long (평균 보상: 3.1)\n",
      "\n",
      "\n",
      "✅ 모든 실험 완료!\n",
      "⏱️  실험 종료 시간: 02:13:03\n"
     ]
    }
   ],
   "source": [
    "# 실험 실행 (원하는 모델만 선택하거나 None으로 모든 모델 테스트)\n",
    "# selected_models = [0, 1, 2]  # 특정 모델들만 테스트\n",
    "selected_models = None  # 모든 모델 테스트\n",
    "\n",
    "# 사용자 정의 시나리오 (필요시 수정)\n",
    "custom_scenarios = [\n",
    "    {'obss_duration': 200, 'ppdu_duration': 33, 'name': 'Extreme OBSS (200)'},\n",
    "    {'obss_duration': 50, 'ppdu_duration': 33, 'name': 'Short OBSS (50)'},\n",
    "]\n",
    "\n",
    "# 실험 실행\n",
    "print(\"⏱️  실험 시작 시간:\", time.strftime('%H:%M:%S'))\n",
    "results = run_comprehensive_comparison(selected_models, custom_scenarios)\n",
    "print(\"\\n✅ 모든 실험 완료!\")\n",
    "print(\"⏱️  실험 종료 시간:\", time.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 DataFrame으로 변환\n",
    "df_results = pd.DataFrame([{\n",
    "    'Scenario': r['scenario'],\n",
    "    'Policy': r['policy_name'],\n",
    "    'Type': r['policy_type'],\n",
    "    'Avg_Reward': r['avg_reward'],\n",
    "    'Std_Reward': r['std_reward'],\n",
    "    'Efficiency': r['efficiency'],\n",
    "    'Stay_Prob': r['action_distribution'][0],\n",
    "    'Switch_Prob': r['action_distribution'][1],\n",
    "    'OBSS_Duration': r['obss_duration'],\n",
    "    'PPDU_Duration': r['ppdu_duration']\n",
    "} for r in results])\n",
    "\n",
    "print(\"📋 실험 결과 요약:\")\n",
    "summary_df = df_results.groupby(['Scenario', 'Policy']).agg({\n",
    "    'Avg_Reward': 'mean',\n",
    "    'Efficiency': 'mean',\n",
    "    'Stay_Prob': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 시각화\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('정책 성능 종합 비교', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. 시나리오별 평균 보상 비교\n",
    "ax1 = axes[0, 0]\n",
    "pivot_reward = df_results.pivot_table(values='Avg_Reward', index='Policy', columns='Scenario', aggfunc='mean')\n",
    "sns.heatmap(pivot_reward, annot=True, fmt='.1f', cmap='RdYlBu_r', ax=ax1, cbar_kws={'label': 'Avg Reward'})\n",
    "ax1.set_title('Average Reward by Policy & Scenario')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# 2. 효율성 비교\n",
    "ax2 = axes[0, 1]\n",
    "pivot_eff = df_results.pivot_table(values='Efficiency', index='Policy', columns='Scenario', aggfunc='mean')\n",
    "sns.heatmap(pivot_eff, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax2, cbar_kws={'label': 'Efficiency'})\n",
    "ax2.set_title('Efficiency by Policy & Scenario')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# 3. 액션 분포\n",
    "ax3 = axes[0, 2]\n",
    "policy_action = df_results.groupby('Policy').agg({\n",
    "    'Stay_Prob': 'mean',\n",
    "    'Switch_Prob': 'mean'\n",
    "})\n",
    "\n",
    "policy_action.plot(kind='bar', stacked=True, ax=ax3, color=['lightblue', 'lightcoral'])\n",
    "ax3.set_title('Action Distribution by Policy')\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.legend(['Stay Primary', 'Switch NPCA'])\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. 베이스라인 vs DRL 성능 비교\n",
    "ax4 = axes[1, 0]\n",
    "type_performance = df_results.groupby(['Type', 'Scenario'])['Avg_Reward'].mean().unstack()\n",
    "type_performance.plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Baseline vs DRL Performance')\n",
    "ax4.set_ylabel('Average Reward')\n",
    "ax4.legend(title='Scenario')\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 5. 정책별 성능 분포 (박스플롯)\n",
    "ax5 = axes[1, 1]\n",
    "top_policies = df_results.groupby('Policy')['Avg_Reward'].mean().nlargest(6).index\n",
    "top_results = df_results[df_results['Policy'].isin(top_policies)]\n",
    "\n",
    "sns.boxplot(data=top_results, x='Policy', y='Avg_Reward', ax=ax5)\n",
    "ax5.set_title('Top 6 Policies Performance Distribution')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. OBSS Duration vs Performance\n",
    "ax6 = axes[1, 2]\n",
    "for policy_type in df_results['Type'].unique():\n",
    "    subset = df_results[df_results['Type'] == policy_type]\n",
    "    avg_by_obss = subset.groupby('OBSS_Duration')['Avg_Reward'].mean()\n",
    "    ax6.plot(avg_by_obss.index, avg_by_obss.values, 'o-', label=policy_type, linewidth=2, markersize=6)\n",
    "\n",
    "ax6.set_title('Performance vs OBSS Duration')\n",
    "ax6.set_xlabel('OBSS Duration (slots)')\n",
    "ax6.set_ylabel('Average Reward')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: 최고 성능 정책 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 최고 성능 정책 찾기\n",
    "overall_best = df_results.loc[df_results['Avg_Reward'].idxmax()]\n",
    "\n",
    "print(\"🏆 전체 최고 성능 정책:\")\n",
    "print(f\"  정책: {overall_best['Policy']}\")\n",
    "print(f\"  시나리오: {overall_best['Scenario']}\")\n",
    "print(f\"  평균 보상: {overall_best['Avg_Reward']:.2f} ± {overall_best['Std_Reward']:.2f}\")\n",
    "print(f\"  효율성: {overall_best['Efficiency']:.4f}\")\n",
    "print(f\"  액션 분포: Stay {overall_best['Stay_Prob']:.2f}, Switch {overall_best['Switch_Prob']:.2f}\")\n",
    "\n",
    "# 시나리오별 최고 성능 정책\n",
    "print(\"\\n🎯 시나리오별 최고 성능:\")\n",
    "for scenario in df_results['Scenario'].unique():\n",
    "    scenario_best = df_results[df_results['Scenario'] == scenario].loc[\n",
    "        df_results[df_results['Scenario'] == scenario]['Avg_Reward'].idxmax()\n",
    "    ]\n",
    "    print(f\"  {scenario:25s}: {scenario_best['Policy']:15s} (보상: {scenario_best['Avg_Reward']:6.1f})\")\n",
    "\n",
    "# 정책 타입별 평균 성능\n",
    "print(\"\\n📊 정책 타입별 평균 성능:\")\n",
    "type_avg = df_results.groupby('Type').agg({\n",
    "    'Avg_Reward': ['mean', 'std'],\n",
    "    'Efficiency': 'mean'\n",
    "})\n",
    "type_avg.columns = ['Avg_Reward_Mean', 'Avg_Reward_Std', 'Efficiency_Mean']\n",
    "\n",
    "for policy_type in type_avg.index:\n",
    "    row = type_avg.loc[policy_type]\n",
    "    print(f\"  {policy_type:10s}: 보상 {row['Avg_Reward_Mean']:6.1f} ± {row['Avg_Reward_Std']:5.1f}, \"\n",
    "          f\"효율성 {row['Efficiency_Mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 CSV로 저장\n",
    "output_dir = Path('./comparison_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "csv_filename = output_dir / f'policy_comparison_{timestamp}.csv'\n",
    "\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "print(f\"📄 결과 저장됨: {csv_filename}\")\n",
    "\n",
    "# 요약 통계 저장\n",
    "summary_filename = output_dir / f'policy_summary_{timestamp}.txt'\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(\"정책 성능 비교 요약\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"전체 최고 성능: {overall_best['Policy']} (보상: {overall_best['Avg_Reward']:.2f})\\n\\n\")\n",
    "    \n",
    "    f.write(\"시나리오별 최고 성능:\\n\")\n",
    "    for scenario in df_results['Scenario'].unique():\n",
    "        scenario_best = df_results[df_results['Scenario'] == scenario].loc[\n",
    "            df_results[df_results['Scenario'] == scenario]['Avg_Reward'].idxmax()\n",
    "        ]\n",
    "        f.write(f\"  {scenario}: {scenario_best['Policy']} (보상: {scenario_best['Avg_Reward']:.1f})\\n\")\n",
    "    \n",
    "    f.write(f\"\\n정책 타입별 평균 성능:\\n\")\n",
    "    for policy_type in type_avg.index:\n",
    "        row = type_avg.loc[policy_type]\n",
    "        f.write(f\"  {policy_type}: 보상 {row['Avg_Reward_Mean']:.1f} ± {row['Avg_Reward_Std']:.1f}\\n\")\n",
    "\n",
    "print(f\"📄 요약 저장됨: {summary_filename}\")\n",
    "print(\"\\n✅ 모든 분석 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 완료!\n",
    "\n",
    "이 노트북에서 다음을 수행했습니다:\n",
    "\n",
    "### ✅ **수행한 작업:**\n",
    "1. **베이스라인 정책** vs **DRL 모델** 종합 비교\n",
    "2. **다양한 시나리오**에서 성능 테스트\n",
    "3. **상세한 성능 지표** 분석 (보상, 효율성, 액션 분포)\n",
    "4. **시각화 및 히트맵**으로 결과 비교\n",
    "5. **결과 저장** (CSV, 요약 텍스트)\n",
    "\n",
    "### 📊 **주요 지표:**\n",
    "- **평균 보상**: 에피소드당 누적 보상\n",
    "- **효율성**: 처리량/총시간 비율\n",
    "- **액션 분포**: Stay vs Switch 선택 비율\n",
    "- **정책 타입 비교**: Baseline vs DRL 성능\n",
    "\n",
    "### 🔧 **커스터마이징:**\n",
    "- **Step 7**: `selected_models`와 `custom_scenarios` 수정하여 원하는 모델/시나리오 테스트\n",
    "- **Step 6**: `test_episodes` 수정하여 테스트 에피소드 수 조정\n",
    "- **Step 5**: 새로운 베이스라인 정책 추가 가능\n",
    "\n",
    "DRL 모델이 베이스라인 정책들과 비교해서 어떤 상황에서 우수한지 명확하게 확인할 수 있습니다! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
