{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì •ì±… ì„±ëŠ¥ ë¹„êµ ë…¸íŠ¸ë¶\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” DRL ëª¨ë¸ê³¼ ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤(Primary-Only, NPCA-Only, Random)ì˜ ì„±ëŠ¥ì„ ë¹„êµë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë“ˆ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\n",
      "PyTorch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤ import\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from drl_framework.network import DQN\n",
    "from drl_framework.configs import PPDU_DURATION_VARIANTS, PPDU_DURATION, RADIO_TRANSITION_TIME, OBSS_GENERATION_RATE\n",
    "from drl_framework.random_access import STA, Simulator, Channel\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "class BaselinePolicies:\n",
    "    \"\"\"ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤ ëª¨ìŒ\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def primary_only(obs_dict):\n",
    "        \"\"\"í•­ìƒ Primary ì±„ë„ì—ì„œ ëŒ€ê¸°\"\"\"\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod \n",
    "    def npca_only(obs_dict):\n",
    "        \"\"\"í•­ìƒ NPCAë¡œ ìŠ¤ìœ„ì¹˜\"\"\"\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_policy(obs_dict):\n",
    "        \"\"\"ëœë¤ ì•¡ì…˜ ì„ íƒ\"\"\"\n",
    "        return np.random.randint(0, 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def smart_threshold(obs_dict, threshold=30):\n",
    "        \"\"\"ì„ê³„ê°’ ê¸°ë°˜ ì •ì±… (OBSS > thresholdì´ë©´ ìŠ¤ìœ„ì¹˜)\"\"\"\n",
    "        obss_remain = obs_dict.get('primary_channel_obss_occupied_remained', 0)\n",
    "        return 1 if obss_remain > threshold else 0\n",
    "\n",
    "print(\"ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: DRL ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– DRL ëª¨ë¸ ë¡œë” ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "class DRLModelLoader:\n",
    "    \"\"\"DRL ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cpu')\n",
    "        self.loaded_models = {}\n",
    "    \n",
    "    def find_models(self, model_dir='./obss_comparison_results'):\n",
    "        \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì°¾ê¸°\"\"\"\n",
    "        model_dir = Path(model_dir)\n",
    "        model_files = list(model_dir.glob('*/model.pth'))\n",
    "        \n",
    "        models_info = []\n",
    "        for model_file in model_files:\n",
    "            try:\n",
    "                checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)\n",
    "                info = {\n",
    "                    'path': model_file,\n",
    "                    'name': model_file.parent.name,\n",
    "                    'obss_duration': checkpoint.get('obss_duration', 'unknown'),\n",
    "                    'ppdu_variant': checkpoint.get('ppdu_variant', 'unknown'),\n",
    "                    'ppdu_duration': checkpoint.get('ppdu_duration', 'unknown'),\n",
    "                    'steps_done': checkpoint.get('steps_done', 'unknown')\n",
    "                }\n",
    "                models_info.append(info)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_file} - {e}\")\n",
    "        \n",
    "        return models_info\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"íŠ¹ì • ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        model_path = str(model_path)\n",
    "        \n",
    "        if model_path in self.loaded_models:\n",
    "            return self.loaded_models[model_path]\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
    "            \n",
    "            # DQN ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "            model = DQN(n_observations=4, n_actions=2).to(self.device)\n",
    "            model.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "            model.eval()\n",
    "            \n",
    "            # ëª¨ë¸ ë˜í¼ ìƒì„±\n",
    "            def drl_policy(obs_dict):\n",
    "                # ê´€ì¸¡ì„ ë²¡í„°ë¡œ ë³€í™˜\n",
    "                obs_vector = [\n",
    "                    obs_dict.get('primary_channel_obss_occupied_remained', 0),\n",
    "                    obs_dict.get('radio_transition_time', 1),\n",
    "                    obs_dict.get('tx_duration', 33),\n",
    "                    obs_dict.get('cw_index', 0)\n",
    "                ]\n",
    "                \n",
    "                # ì˜ˆì¸¡\n",
    "                input_tensor = torch.tensor(obs_vector, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(input_tensor)\n",
    "                    action = q_values.argmax(dim=1).item()\n",
    "                return action\n",
    "            \n",
    "            self.loaded_models[model_path] = {\n",
    "                'model': model,\n",
    "                'policy': drl_policy,\n",
    "                'info': checkpoint\n",
    "            }\n",
    "            \n",
    "            return self.loaded_models[model_path]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "# ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”\n",
    "model_loader = DRLModelLoader()\n",
    "print(\"ğŸ¤– DRL ëª¨ë¸ ë¡œë” ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ë°œê²¬ëœ ëª¨ë¸ ìˆ˜: 14\n",
      "\n",
      "ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:\n",
      "                        name  obss_duration ppdu_variant ppdu_duration  steps_done\n",
      "1          ppdu_long_obss_50             50         long            50         630\n",
      "2         ppdu_short_obss_50             50        short            20        1658\n",
      "3         ppdu_long_obss_200            200         long            50        3084\n",
      "4      trained_model_obss_50             50      unknown       unknown        5014\n",
      "5   ppdu_extra_long_obss_100            100   extra_long            80        1149\n",
      "6     trained_model_obss_100            100      unknown       unknown        5170\n",
      "7         ppdu_long_obss_100            100         long            50        1488\n",
      "8        ppdu_short_obss_100            100        short            20        3246\n",
      "9   ppdu_extra_long_obss_200            200   extra_long            80        2220\n",
      "10      ppdu_medium_obss_100            100       medium            33        1803\n",
      "11       ppdu_medium_obss_50             50       medium            33        1151\n",
      "12      ppdu_medium_obss_200            200       medium            33        4413\n",
      "13       ppdu_short_obss_200            200        short            20        6159\n",
      "14   ppdu_extra_long_obss_50             50   extra_long            80         566\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì°¾ê¸°\n",
    "available_models = model_loader.find_models()\n",
    "\n",
    "print(f\"ğŸ” ë°œê²¬ëœ ëª¨ë¸ ìˆ˜: {len(available_models)}\")\n",
    "print(\"\\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:\")\n",
    "\n",
    "models_df = pd.DataFrame(available_models)\n",
    "if not models_df.empty:\n",
    "    display_df = models_df[['name', 'obss_duration', 'ppdu_variant', 'ppdu_duration', 'steps_done']].copy()\n",
    "    display_df.index = range(1, len(display_df) + 1)\n",
    "    print(display_df.to_string())\n",
    "else:\n",
    "    print(\"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def test_policy_performance(policy_func, policy_name, test_episodes=30, \n",
    "                          obss_duration=100, ppdu_duration=33, verbose=False):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ì •ì±…ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    Args:\n",
    "        policy_func: í…ŒìŠ¤íŠ¸í•  ì •ì±… í•¨ìˆ˜\n",
    "        policy_name: ì •ì±… ì´ë¦„\n",
    "        test_episodes: í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜\n",
    "        obss_duration: OBSS ì§€ì† ì‹œê°„\n",
    "        ppdu_duration: PPDU ì§€ì† ì‹œê°„\n",
    "        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        dict: ì„±ëŠ¥ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # ì±„ë„ ì„¤ì •\n",
    "    channels = [\n",
    "        Channel(channel_id=0, obss_generation_rate=OBSS_GENERATION_RATE['secondary']),  # NPCA channel\n",
    "        Channel(channel_id=1, obss_generation_rate=0.01, obss_duration_range=(obss_duration, obss_duration))  # Primary channel\n",
    "    ]\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ\n",
    "    episode_rewards = []\n",
    "    action_counts = [0, 0]  # [Stay, Switch]\n",
    "    total_throughput = 0\n",
    "    decision_count = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(test_episodes):\n",
    "        # STA ì„¤ì •\n",
    "        sta = STA(\n",
    "            sta_id=0,\n",
    "            channel_id=1,\n",
    "            primary_channel=channels[1],\n",
    "            npca_channel=channels[0],\n",
    "            npca_enabled=True,\n",
    "            ppdu_duration=ppdu_duration,\n",
    "            radio_transition_time=RADIO_TRANSITION_TIME\n",
    "        )\n",
    "        \n",
    "        # ì •ì±… ì„¤ì •\n",
    "        if hasattr(policy_func, '__call__'):\n",
    "            # ë² ì´ìŠ¤ë¼ì¸ ì •ì±…\n",
    "            def create_fixed_policy(policy_func):\n",
    "                def fixed_action():\n",
    "                    obs_dict = sta.get_obs()\n",
    "                    action = policy_func(obs_dict)\n",
    "                    \n",
    "                    # ì•¡ì…˜ ì¹´ìš´íŠ¸\n",
    "                    action_counts[action] += 1\n",
    "                    \n",
    "                    return action\n",
    "                return fixed_action\n",
    "            sta._fixed_action = create_fixed_policy(policy_func)\n",
    "        \n",
    "        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
    "        simulator = Simulator(num_slots:=300, channels=channels, stas=[sta])\n",
    "        simulator.device = device\n",
    "        simulator.run()\n",
    "        \n",
    "        # ê²°ê³¼ ìˆ˜ì§‘\n",
    "        episode_reward = sta.episode_reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "        total_throughput += sta.channel_occupancy_time\n",
    "        \n",
    "        if verbose and episode < 3:\n",
    "            print(f\"  Episode {episode}: Reward={episode_reward:.1f}, Throughput={sta.channel_occupancy_time}\")\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    # ê²°ê³¼ ê³„ì‚°\n",
    "    avg_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    total_actions = sum(action_counts)\n",
    "    \n",
    "    if total_actions > 0:\n",
    "        action_probs = [count/total_actions for count in action_counts]\n",
    "    else:\n",
    "        action_probs = [0.5, 0.5]\n",
    "    \n",
    "    efficiency = total_throughput / (test_episodes * num_slots)  # ì´ ìŠ¬ë¡¯ ëŒ€ë¹„ ì²˜ë¦¬ëŸ‰\n",
    "    \n",
    "    result = {\n",
    "        'policy_name': policy_name,\n",
    "        'avg_reward': avg_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'total_throughput': total_throughput,\n",
    "        'efficiency': efficiency,\n",
    "        'action_distribution': action_probs,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'test_time': test_time,\n",
    "        'obss_duration': obss_duration,\n",
    "        'ppdu_duration': ppdu_duration\n",
    "    }\n",
    "    \n",
    "    if not verbose:\n",
    "        print(f\"  {policy_name:12s}: Avg Reward = {avg_reward:6.1f} Â± {std_reward:5.1f}, \"\n",
    "              f\"Efficiency = {efficiency:.4f}, Stay/Switch = {action_probs[0]:.2f}/{action_probs[1]:.2f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"ğŸ§ª ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: ì¢…í•© ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ì¢…í•© ë¹„êµ ì‹¤í—˜ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def run_comprehensive_comparison(selected_models_idx=None, test_scenarios=None):\n",
    "    \"\"\"\n",
    "    ì¢…í•©ì ì¸ ì •ì±… ë¹„êµ ì‹¤í—˜\n",
    "    \n",
    "    Args:\n",
    "        selected_models_idx: í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ëª¨ë¸)\n",
    "        test_scenarios: í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_scenarios is None:\n",
    "        test_scenarios = [\n",
    "            {'obss_duration': 50, 'ppdu_duration': 33, 'name': 'Short OBSS'},\n",
    "            {'obss_duration': 150, 'ppdu_duration': 33, 'name': 'Long OBSS'},\n",
    "            {'obss_duration': 200, 'ppdu_duration': 20, 'name': 'Extreme OBSS, Short PPDU'},\n",
    "            {'obss_duration': 200, 'ppdu_duration': 80, 'name': 'Extreme OBSS, Long PPDU'},\n",
    "        ]\n",
    "    \n",
    "    print(\"ğŸš€ ì¢…í•© ë¹„êµ ì‹¤í—˜ ì‹œì‘...\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for scenario_idx, scenario in enumerate(test_scenarios):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx + 1}: {scenario['name']}\")\n",
    "        print(f\"OBSS Duration: {scenario['obss_duration']}, PPDU Duration: {scenario['ppdu_duration']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        scenario_results = []\n",
    "        \n",
    "        # ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤ í…ŒìŠ¤íŠ¸\n",
    "        baseline_policies = [\n",
    "            (BaselinePolicies.primary_only, 'Primary-Only'),\n",
    "            (BaselinePolicies.npca_only, 'NPCA-Only'),\n",
    "            (BaselinePolicies.random_policy, 'Random'),\n",
    "            (lambda obs: BaselinePolicies.smart_threshold(obs, 30), 'Smart-30'),\n",
    "            (lambda obs: BaselinePolicies.smart_threshold(obs, 100), 'Smart-100')\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤:\")\n",
    "        for policy_func, policy_name in baseline_policies:\n",
    "            result = test_policy_performance(\n",
    "                policy_func, policy_name,\n",
    "                test_episodes=200,\n",
    "                obss_duration=scenario['obss_duration'],\n",
    "                ppdu_duration=scenario['ppdu_duration']\n",
    "            )\n",
    "            result['scenario'] = scenario['name']\n",
    "            result['policy_type'] = 'Baseline'\n",
    "            scenario_results.append(result)\n",
    "        \n",
    "        # DRL ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\nğŸ¤– DRL ëª¨ë¸ë“¤:\")\n",
    "        \n",
    "        models_to_test = available_models\n",
    "        if selected_models_idx is not None:\n",
    "            models_to_test = [available_models[i] for i in selected_models_idx if i < len(available_models)]\n",
    "        \n",
    "        for model_info in models_to_test[:5]:  # ìµœëŒ€ 5ê°œ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸\n",
    "            loaded_model = model_loader.load_model(model_info['path'])\n",
    "            if loaded_model:\n",
    "                model_name = f\"DRL-{model_info['ppdu_variant']}\"\n",
    "                result = test_policy_performance(\n",
    "                    loaded_model['policy'], model_name,\n",
    "                    test_episodes=200,\n",
    "                    obss_duration=scenario['obss_duration'],\n",
    "                    ppdu_duration=scenario['ppdu_duration']\n",
    "                )\n",
    "                result['scenario'] = scenario['name']\n",
    "                result['policy_type'] = 'DRL'\n",
    "                result['model_info'] = model_info\n",
    "                scenario_results.append(result)\n",
    "        \n",
    "        all_results.extend(scenario_results)\n",
    "        \n",
    "        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœê³  ì„±ëŠ¥ ì¶œë ¥\n",
    "        best_result = max(scenario_results, key=lambda x: x['avg_reward'])\n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥: {best_result['policy_name']} (í‰ê·  ë³´ìƒ: {best_result['avg_reward']:.1f})\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"ğŸ¯ ì¢…í•© ë¹„êµ ì‹¤í—˜ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: ì‹¤í—˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  ì‹¤í—˜ ì‹œì‘ ì‹œê°„: 02:12:58\n",
      "ğŸš€ ì¢…í•© ë¹„êµ ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "============================================================\n",
      "ì‹œë‚˜ë¦¬ì˜¤ 1: Extreme OBSS (200)\n",
      "OBSS Duration: 200, PPDU Duration: 33\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤:\n",
      "  Primary-Only: Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  NPCA-Only   : Avg Reward =    2.3 Â±  23.0, Efficiency = 0.0077, Stay/Switch = 0.00/1.00\n",
      "  Random      : Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  Smart-30    : Avg Reward =  195.5 Â±  23.8, Efficiency = 0.6518, Stay/Switch = 0.00/1.00\n",
      "  Smart-100   : Avg Reward =  195.9 Â±  23.8, Efficiency = 0.6529, Stay/Switch = 0.00/1.00\n",
      "\n",
      "ğŸ¤– DRL ëª¨ë¸ë“¤:\n",
      "  DRL-long    : Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.00/1.00\n",
      "  DRL-short   : Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  DRL-long    : Avg Reward =   55.8 Â±  22.5, Efficiency = 0.1859, Stay/Switch = 1.00/0.00\n",
      "  DRL-unknown : Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  DRL-extra_long: Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥: Smart-100 (í‰ê·  ë³´ìƒ: 195.9)\n",
      "\n",
      "============================================================\n",
      "ì‹œë‚˜ë¦¬ì˜¤ 2: Short OBSS (50)\n",
      "OBSS Duration: 50, PPDU Duration: 33\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤:\n",
      "  Primary-Only: Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  NPCA-Only   : Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.00/1.00\n",
      "  Random      : Avg Reward =    1.0 Â±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  Smart-30    : Avg Reward =    1.0 Â±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  Smart-100   : Avg Reward =    1.3 Â±  13.5, Efficiency = 0.0044, Stay/Switch = 1.00/0.00\n",
      "\n",
      "ğŸ¤– DRL ëª¨ë¸ë“¤:\n",
      "  DRL-long    : Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "  DRL-short   : Avg Reward =    1.0 Â±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  DRL-long    : Avg Reward =    3.1 Â±  25.7, Efficiency = 0.0104, Stay/Switch = 1.00/0.00\n",
      "  DRL-unknown : Avg Reward =    1.0 Â±  14.0, Efficiency = 0.0033, Stay/Switch = 0.50/0.50\n",
      "  DRL-extra_long: Avg Reward =    1.2 Â±  16.3, Efficiency = 0.0039, Stay/Switch = 0.50/0.50\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥: DRL-long (í‰ê·  ë³´ìƒ: 3.1)\n",
      "\n",
      "\n",
      "âœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!\n",
      "â±ï¸  ì‹¤í—˜ ì¢…ë£Œ ì‹œê°„: 02:13:03\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í—˜ ì‹¤í–‰ (ì›í•˜ëŠ” ëª¨ë¸ë§Œ ì„ íƒí•˜ê±°ë‚˜ Noneìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸)\n",
    "# selected_models = [0, 1, 2]  # íŠ¹ì • ëª¨ë¸ë“¤ë§Œ í…ŒìŠ¤íŠ¸\n",
    "selected_models = None  # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ì‹œë‚˜ë¦¬ì˜¤ (í•„ìš”ì‹œ ìˆ˜ì •)\n",
    "custom_scenarios = [\n",
    "    {'obss_duration': 200, 'ppdu_duration': 33, 'name': 'Extreme OBSS (200)'},\n",
    "    {'obss_duration': 50, 'ppdu_duration': 33, 'name': 'Short OBSS (50)'},\n",
    "]\n",
    "\n",
    "# ì‹¤í—˜ ì‹¤í–‰\n",
    "print(\"â±ï¸  ì‹¤í—˜ ì‹œì‘ ì‹œê°„:\", time.strftime('%H:%M:%S'))\n",
    "results = run_comprehensive_comparison(selected_models, custom_scenarios)\n",
    "print(\"\\nâœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(\"â±ï¸  ì‹¤í—˜ ì¢…ë£Œ ì‹œê°„:\", time.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df_results = pd.DataFrame([{\n",
    "    'Scenario': r['scenario'],\n",
    "    'Policy': r['policy_name'],\n",
    "    'Type': r['policy_type'],\n",
    "    'Avg_Reward': r['avg_reward'],\n",
    "    'Std_Reward': r['std_reward'],\n",
    "    'Efficiency': r['efficiency'],\n",
    "    'Stay_Prob': r['action_distribution'][0],\n",
    "    'Switch_Prob': r['action_distribution'][1],\n",
    "    'OBSS_Duration': r['obss_duration'],\n",
    "    'PPDU_Duration': r['ppdu_duration']\n",
    "} for r in results])\n",
    "\n",
    "print(\"ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:\")\n",
    "summary_df = df_results.groupby(['Scenario', 'Policy']).agg({\n",
    "    'Avg_Reward': 'mean',\n",
    "    'Efficiency': 'mean',\n",
    "    'Stay_Prob': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¢…í•© ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('ì •ì±… ì„±ëŠ¥ ì¢…í•© ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ë³´ìƒ ë¹„êµ\n",
    "ax1 = axes[0, 0]\n",
    "pivot_reward = df_results.pivot_table(values='Avg_Reward', index='Policy', columns='Scenario', aggfunc='mean')\n",
    "sns.heatmap(pivot_reward, annot=True, fmt='.1f', cmap='RdYlBu_r', ax=ax1, cbar_kws={'label': 'Avg Reward'})\n",
    "ax1.set_title('Average Reward by Policy & Scenario')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# 2. íš¨ìœ¨ì„± ë¹„êµ\n",
    "ax2 = axes[0, 1]\n",
    "pivot_eff = df_results.pivot_table(values='Efficiency', index='Policy', columns='Scenario', aggfunc='mean')\n",
    "sns.heatmap(pivot_eff, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax2, cbar_kws={'label': 'Efficiency'})\n",
    "ax2.set_title('Efficiency by Policy & Scenario')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# 3. ì•¡ì…˜ ë¶„í¬\n",
    "ax3 = axes[0, 2]\n",
    "policy_action = df_results.groupby('Policy').agg({\n",
    "    'Stay_Prob': 'mean',\n",
    "    'Switch_Prob': 'mean'\n",
    "})\n",
    "\n",
    "policy_action.plot(kind='bar', stacked=True, ax=ax3, color=['lightblue', 'lightcoral'])\n",
    "ax3.set_title('Action Distribution by Policy')\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.legend(['Stay Primary', 'Switch NPCA'])\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. ë² ì´ìŠ¤ë¼ì¸ vs DRL ì„±ëŠ¥ ë¹„êµ\n",
    "ax4 = axes[1, 0]\n",
    "type_performance = df_results.groupby(['Type', 'Scenario'])['Avg_Reward'].mean().unstack()\n",
    "type_performance.plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Baseline vs DRL Performance')\n",
    "ax4.set_ylabel('Average Reward')\n",
    "ax4.legend(title='Scenario')\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 5. ì •ì±…ë³„ ì„±ëŠ¥ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)\n",
    "ax5 = axes[1, 1]\n",
    "top_policies = df_results.groupby('Policy')['Avg_Reward'].mean().nlargest(6).index\n",
    "top_results = df_results[df_results['Policy'].isin(top_policies)]\n",
    "\n",
    "sns.boxplot(data=top_results, x='Policy', y='Avg_Reward', ax=ax5)\n",
    "ax5.set_title('Top 6 Policies Performance Distribution')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. OBSS Duration vs Performance\n",
    "ax6 = axes[1, 2]\n",
    "for policy_type in df_results['Type'].unique():\n",
    "    subset = df_results[df_results['Type'] == policy_type]\n",
    "    avg_by_obss = subset.groupby('OBSS_Duration')['Avg_Reward'].mean()\n",
    "    ax6.plot(avg_by_obss.index, avg_by_obss.values, 'o-', label=policy_type, linewidth=2, markersize=6)\n",
    "\n",
    "ax6.set_title('Performance vs OBSS Duration')\n",
    "ax6.set_xlabel('OBSS Duration (slots)')\n",
    "ax6.set_ylabel('Average Reward')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ìµœê³  ì„±ëŠ¥ ì •ì±… ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ìµœê³  ì„±ëŠ¥ ì •ì±… ì°¾ê¸°\n",
    "overall_best = df_results.loc[df_results['Avg_Reward'].idxmax()]\n",
    "\n",
    "print(\"ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥ ì •ì±…:\")\n",
    "print(f\"  ì •ì±…: {overall_best['Policy']}\")\n",
    "print(f\"  ì‹œë‚˜ë¦¬ì˜¤: {overall_best['Scenario']}\")\n",
    "print(f\"  í‰ê·  ë³´ìƒ: {overall_best['Avg_Reward']:.2f} Â± {overall_best['Std_Reward']:.2f}\")\n",
    "print(f\"  íš¨ìœ¨ì„±: {overall_best['Efficiency']:.4f}\")\n",
    "print(f\"  ì•¡ì…˜ ë¶„í¬: Stay {overall_best['Stay_Prob']:.2f}, Switch {overall_best['Switch_Prob']:.2f}\")\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœê³  ì„±ëŠ¥ ì •ì±…\n",
    "print(\"\\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœê³  ì„±ëŠ¥:\")\n",
    "for scenario in df_results['Scenario'].unique():\n",
    "    scenario_best = df_results[df_results['Scenario'] == scenario].loc[\n",
    "        df_results[df_results['Scenario'] == scenario]['Avg_Reward'].idxmax()\n",
    "    ]\n",
    "    print(f\"  {scenario:25s}: {scenario_best['Policy']:15s} (ë³´ìƒ: {scenario_best['Avg_Reward']:6.1f})\")\n",
    "\n",
    "# ì •ì±… íƒ€ì…ë³„ í‰ê·  ì„±ëŠ¥\n",
    "print(\"\\nğŸ“Š ì •ì±… íƒ€ì…ë³„ í‰ê·  ì„±ëŠ¥:\")\n",
    "type_avg = df_results.groupby('Type').agg({\n",
    "    'Avg_Reward': ['mean', 'std'],\n",
    "    'Efficiency': 'mean'\n",
    "})\n",
    "type_avg.columns = ['Avg_Reward_Mean', 'Avg_Reward_Std', 'Efficiency_Mean']\n",
    "\n",
    "for policy_type in type_avg.index:\n",
    "    row = type_avg.loc[policy_type]\n",
    "    print(f\"  {policy_type:10s}: ë³´ìƒ {row['Avg_Reward_Mean']:6.1f} Â± {row['Avg_Reward_Std']:5.1f}, \"\n",
    "          f\"íš¨ìœ¨ì„± {row['Efficiency_Mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥\n",
    "output_dir = Path('./comparison_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "csv_filename = output_dir / f'policy_comparison_{timestamp}.csv'\n",
    "\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "print(f\"ğŸ“„ ê²°ê³¼ ì €ì¥ë¨: {csv_filename}\")\n",
    "\n",
    "# ìš”ì•½ í†µê³„ ì €ì¥\n",
    "summary_filename = output_dir / f'policy_summary_{timestamp}.txt'\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(\"ì •ì±… ì„±ëŠ¥ ë¹„êµ ìš”ì•½\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"ì „ì²´ ìµœê³  ì„±ëŠ¥: {overall_best['Policy']} (ë³´ìƒ: {overall_best['Avg_Reward']:.2f})\\n\\n\")\n",
    "    \n",
    "    f.write(\"ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœê³  ì„±ëŠ¥:\\n\")\n",
    "    for scenario in df_results['Scenario'].unique():\n",
    "        scenario_best = df_results[df_results['Scenario'] == scenario].loc[\n",
    "            df_results[df_results['Scenario'] == scenario]['Avg_Reward'].idxmax()\n",
    "        ]\n",
    "        f.write(f\"  {scenario}: {scenario_best['Policy']} (ë³´ìƒ: {scenario_best['Avg_Reward']:.1f})\\n\")\n",
    "    \n",
    "    f.write(f\"\\nì •ì±… íƒ€ì…ë³„ í‰ê·  ì„±ëŠ¥:\\n\")\n",
    "    for policy_type in type_avg.index:\n",
    "        row = type_avg.loc[policy_type]\n",
    "        f.write(f\"  {policy_type}: ë³´ìƒ {row['Avg_Reward_Mean']:.1f} Â± {row['Avg_Reward_Std']:.1f}\\n\")\n",
    "\n",
    "print(f\"ğŸ“„ ìš”ì•½ ì €ì¥ë¨: {summary_filename}\")\n",
    "print(\"\\nâœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ ì™„ë£Œ!\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ìŒì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "### âœ… **ìˆ˜í–‰í•œ ì‘ì—…:**\n",
    "1. **ë² ì´ìŠ¤ë¼ì¸ ì •ì±…** vs **DRL ëª¨ë¸** ì¢…í•© ë¹„êµ\n",
    "2. **ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤**ì—ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "3. **ìƒì„¸í•œ ì„±ëŠ¥ ì§€í‘œ** ë¶„ì„ (ë³´ìƒ, íš¨ìœ¨ì„±, ì•¡ì…˜ ë¶„í¬)\n",
    "4. **ì‹œê°í™” ë° íˆíŠ¸ë§µ**ìœ¼ë¡œ ê²°ê³¼ ë¹„êµ\n",
    "5. **ê²°ê³¼ ì €ì¥** (CSV, ìš”ì•½ í…ìŠ¤íŠ¸)\n",
    "\n",
    "### ğŸ“Š **ì£¼ìš” ì§€í‘œ:**\n",
    "- **í‰ê·  ë³´ìƒ**: ì—í”¼ì†Œë“œë‹¹ ëˆ„ì  ë³´ìƒ\n",
    "- **íš¨ìœ¨ì„±**: ì²˜ë¦¬ëŸ‰/ì´ì‹œê°„ ë¹„ìœ¨\n",
    "- **ì•¡ì…˜ ë¶„í¬**: Stay vs Switch ì„ íƒ ë¹„ìœ¨\n",
    "- **ì •ì±… íƒ€ì… ë¹„êµ**: Baseline vs DRL ì„±ëŠ¥\n",
    "\n",
    "### ğŸ”§ **ì»¤ìŠ¤í„°ë§ˆì´ì§•:**\n",
    "- **Step 7**: `selected_models`ì™€ `custom_scenarios` ìˆ˜ì •í•˜ì—¬ ì›í•˜ëŠ” ëª¨ë¸/ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\n",
    "- **Step 6**: `test_episodes` ìˆ˜ì •í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ ì¡°ì •\n",
    "- **Step 5**: ìƒˆë¡œìš´ ë² ì´ìŠ¤ë¼ì¸ ì •ì±… ì¶”ê°€ ê°€ëŠ¥\n",
    "\n",
    "DRL ëª¨ë¸ì´ ë² ì´ìŠ¤ë¼ì¸ ì •ì±…ë“¤ê³¼ ë¹„êµí•´ì„œ ì–´ë–¤ ìƒí™©ì—ì„œ ìš°ìˆ˜í•œì§€ ëª…í™•í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
