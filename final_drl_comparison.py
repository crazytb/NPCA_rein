#!/usr/bin/env python3
"""
ìµœì¢… DRL vs ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ë¹„êµ
ìƒˆë¡œìš´ ë³´ìƒ êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ DRL ì •ì±…, Always Primary, Always NPCA ë¹„êµ
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from drl_framework.random_access import STA, Channel, Simulator
from drl_framework.network import DQN

def create_environment(num_slots=200, num_stas=10):
    """ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ìƒì„±"""
    # Primary ì±„ë„: OBSS ë°œìƒ
    primary_channel = Channel(
        channel_id=0,
        obss_generation_rate=0.3,  
        obss_duration_range=(20, 40)
    )
    
    # NPCA ì±„ë„: OBSS ì—†ìŒ  
    npca_channel = Channel(
        channel_id=1,
        obss_generation_rate=0.0,  
        obss_duration_range=(0, 0)
    )
    
    channels = [primary_channel, npca_channel]
    
    # STA ì„¤ì •
    stas = []
    for i in range(num_stas):
        sta = STA(
            sta_id=i,
            channel_id=0,
            primary_channel=primary_channel,
            npca_channel=npca_channel,
            npca_enabled=True
        )
        stas.append(sta)
    
    return Simulator(
        channels=channels,
        stas=stas,
        num_slots=num_slots
    )

def run_drl_test(model_path, num_episodes=50, num_slots=200, num_stas=10):
    """DRL ì •ì±… í…ŒìŠ¤íŠ¸ - ì‹œë®¬ë ˆì´í„° ì§ì ‘ ì‚¬ìš©"""
    device = torch.device("cuda" if torch.cuda.is_available() else 
                         "mps" if torch.backends.mps.is_available() else "cpu")
    
    # ëª¨ë¸ ë¡œë“œ
    model = DQN(n_observations=4, n_actions=2).to(device)
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['policy_net_state_dict'])
    model.eval()
    
    episode_rewards = []
    
    for episode in range(num_episodes):
        simulator = create_environment(num_slots, num_stas)
        
        # DRL ì •ì±…ì„ ìœ„í•œ ê°„ì†Œí™”ëœ Learner ì„¤ì •
        class SimpleLearner:
            def __init__(self, policy_net, device):
                self.policy_net = policy_net
                self.device = device
                self.steps_done = 0
                self.memory = None  # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš© ì•ˆí•¨
            
            def select_action(self, state_tensor, training=False):
                """Greedy ì•¡ì…˜ ì„ íƒ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"""
                with torch.no_grad():
                    if state_tensor.dim() == 1:
                        state_tensor = state_tensor.unsqueeze(0)
                    q_values = self.policy_net(state_tensor)
                    return q_values.max(1)[1].item()
        
        # ëª¨ë“  NPCA enabled STAì— DRL ì •ì±… í• ë‹¹
        for sta in simulator.stas:
            if hasattr(sta, 'npca_enabled') and sta.npca_enabled:
                sta.learner = SimpleLearner(model, device)
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        simulator.run()
        
        # ì´ ì„±ê³µ ì „ì†¡ ìŠ¬ë¡¯ ìˆ˜ ê³„ì‚°
        total_reward = sum(sta.episode_reward for sta in simulator.stas)
        episode_rewards.append(total_reward)
        
        if episode % 10 == 0:
            print(f"  Episode {episode}: DRL Total Successful Slots = {total_reward:.1f}")
    
    return episode_rewards

def run_baseline_test(strategy, num_episodes=50, num_slots=200, num_stas=10):
    """ë² ì´ìŠ¤ë¼ì¸ ì „ëµ í…ŒìŠ¤íŠ¸"""
    episode_rewards = []
    
    for episode in range(num_episodes):
        simulator = create_environment(num_slots, num_stas)
        
        # ê³ ì • ì „ëµ ì„¤ì •
        for sta in simulator.stas:
            if hasattr(sta, 'npca_enabled') and sta.npca_enabled:
                if strategy == "always_primary":
                    sta._fixed_action = 0  
                elif strategy == "always_npca":
                    sta._fixed_action = 1  
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        simulator.run()
        
        # ì´ ì„±ê³µ ì „ì†¡ ìŠ¬ë¡¯ ìˆ˜ ê³„ì‚°
        total_reward = sum(sta.episode_reward for sta in simulator.stas)
        episode_rewards.append(total_reward)
        
        if episode % 10 == 0:
            print(f"  Episode {episode}: {strategy} Total Successful Slots = {total_reward:.1f}")
    
    return episode_rewards

def plot_comparison(drl_rewards, primary_rewards, npca_rewards, save_dir="./final_comparison"):
    """ìµœì¢… ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
    os.makedirs(save_dir, exist_ok=True)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. ì—í”¼ì†Œë“œë³„ ë³´ìƒ ë¹„êµ
    episodes = range(len(drl_rewards))
    ax1.plot(episodes, drl_rewards, label='DRL Policy', alpha=0.8, color='blue', linewidth=1.5)
    ax1.plot(episodes, primary_rewards, label='Always Primary', alpha=0.8, color='red', linewidth=1.5)
    ax1.plot(episodes, npca_rewards, label='Always NPCA', alpha=0.8, color='green', linewidth=1.5)
    ax1.set_title('Episode Performance: Successful Transmission Slots', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Successful Transmission Slots')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì´ë™ í‰ê· 
    if len(drl_rewards) >= 10:
        window = 10
        drl_ma = np.convolve(drl_rewards, np.ones(window)/window, mode='valid')
        primary_ma = np.convolve(primary_rewards, np.ones(window)/window, mode='valid')
        npca_ma = np.convolve(npca_rewards, np.ones(window)/window, mode='valid')
        episodes_ma = range(window-1, len(drl_rewards))
        
        ax2.plot(episodes_ma, drl_ma, label='DRL Policy', color='blue', linewidth=2)
        ax2.plot(episodes_ma, primary_ma, label='Always Primary', color='red', linewidth=2)
        ax2.plot(episodes_ma, npca_ma, label='Always NPCA', color='green', linewidth=2)
    
    ax2.set_title('Moving Average Performance (Window=10)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Average Successful Transmission Slots')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ë³´ìƒ ë¶„í¬
    data = [drl_rewards, primary_rewards, npca_rewards]
    labels = ['DRL Policy', 'Always Primary', 'Always NPCA']
    box_plot = ax3.boxplot(data, tick_labels=labels, patch_artist=True)
    
    # ë°•ìŠ¤í”Œë¡¯ ìƒ‰ìƒ ì„¤ì •
    colors = ['lightblue', 'lightcoral', 'lightgreen']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
    
    ax3.set_title('Performance Distribution', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Successful Transmission Slots')
    ax3.grid(True, alpha=0.3)
    
    # 4. í‰ê·  ì„±ëŠ¥ ë°” ê·¸ë˜í”„
    means = [np.mean(drl_rewards), np.mean(primary_rewards), np.mean(npca_rewards)]
    stds = [np.std(drl_rewards), np.std(primary_rewards), np.std(npca_rewards)]
    
    bars = ax4.bar(labels, means, yerr=stds, capsize=5, 
                   color=['blue', 'red', 'green'], alpha=0.7, 
                   edgecolor='black', linewidth=1)
    ax4.set_title('Average Performance Comparison', fontsize=14, fontweight='bold')
    ax4.set_ylabel('Mean Successful Transmission Slots')
    ax4.grid(True, alpha=0.3, axis='y')
    
    # ë°” ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
    for bar, mean, std in zip(bars, means, stds):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + std + 2,
                f'{mean:.1f}Â±{std:.1f}',
                ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/final_drl_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 70)
    print("ìµœì¢… DRL vs ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ë¹„êµ")
    print("ìƒˆë¡œìš´ ë³´ìƒ êµ¬ì¡°: ì„±ê³µì ìœ¼ë¡œ ì „ì†¡í•œ ì´ ìŠ¬ë¡¯ ìˆ˜")
    print("=" * 70)
    
    model_path = "./semi_mdp_results/semi_mdp_model.pth"
    num_episodes = 50
    num_slots = 200
    num_stas = 10
    results_dir = "./final_comparison"
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("main_semi_mdp_training.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ì„¸ìš”.")
        return
    
    os.makedirs(results_dir, exist_ok=True)
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì„¤ì •:")
    print(f"  - ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
    print(f"  - ì—í”¼ì†Œë“œë‹¹ ìŠ¬ë¡¯ ìˆ˜: {num_slots}")
    print(f"  - NPCA enabled STA ìˆ˜: {num_stas}")
    print()
    
    # 1. DRL ì •ì±… í…ŒìŠ¤íŠ¸
    print("ğŸ¤– 1. DRL ì •ì±… í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        drl_rewards = run_drl_test(model_path, num_episodes, num_slots, num_stas)
        print(f"   âœ… DRL í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    except Exception as e:
        print(f"   âŒ DRL í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return
    
    # 2. Always Primary í…ŒìŠ¤íŠ¸
    print("\nğŸ”´ 2. Always Primary ì „ëµ í…ŒìŠ¤íŠ¸ ì¤‘...")
    primary_rewards = run_baseline_test("always_primary", num_episodes, num_slots, num_stas)
    print(f"   âœ… Always Primary í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # 3. Always NPCA í…ŒìŠ¤íŠ¸
    print("\nğŸŸ¢ 3. Always NPCA ì „ëµ í…ŒìŠ¤íŠ¸ ì¤‘...")
    npca_rewards = run_baseline_test("always_npca", num_episodes, num_slots, num_stas)
    print(f"   âœ… Always NPCA í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # 4. ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame({
        'DRL_Policy': drl_rewards,
        'Always_Primary': primary_rewards,
        'Always_NPCA': npca_rewards
    })
    results_df.to_csv(f"{results_dir}/final_comparison_results.csv", index=False)
    
    # 5. ì‹œê°í™”
    print("\nğŸ“ˆ 4. ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    plot_comparison(drl_rewards, primary_rewards, npca_rewards, results_dir)
    
    # 6. í†µê³„ ë¶„ì„ ë° ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ† ìµœì¢… ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼")
    print("=" * 70)
    
    strategies = {
        'DRL Policy': np.array(drl_rewards),
        'Always Primary': np.array(primary_rewards),
        'Always NPCA': np.array(npca_rewards)
    }
    
    # ê¸°ë³¸ í†µê³„
    print("ğŸ“Š ê¸°ë³¸ í†µê³„:")
    for name, rewards in strategies.items():
        print(f"  {name:15s} - Mean: {rewards.mean():6.2f}, "
              f"Std: {rewards.std():5.2f}, "
              f"Max: {rewards.max():6.1f}, "
              f"Min: {rewards.min():6.1f}")
    
    # ì„±ëŠ¥ ë¹„êµ
    print("\nğŸš€ ì„±ëŠ¥ ê°œì„ ìœ¨:")
    drl_mean = strategies['DRL Policy'].mean()
    primary_mean = strategies['Always Primary'].mean()
    npca_mean = strategies['Always NPCA'].mean()
    
    drl_vs_primary = ((drl_mean - primary_mean) / primary_mean * 100)
    drl_vs_npca = ((drl_mean - npca_mean) / npca_mean * 100)
    
    print(f"  DRL vs Always Primary : {drl_vs_primary:+6.1f}%")
    print(f"  DRL vs Always NPCA    : {drl_vs_npca:+6.1f}%")
    
    # ìµœê³  ì„±ëŠ¥ ì „ëµ
    best_strategy = max(strategies.items(), key=lambda x: x[1].mean())
    print(f"\nğŸ… ìµœê³  ì„±ëŠ¥ ì „ëµ: {best_strategy[0]} (í‰ê·  {best_strategy[1].mean():.2f} ìŠ¬ë¡¯)")
    
    print(f"\nğŸ’¾ ëª¨ë“  ê²°ê³¼ê°€ {results_dir}/ ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   - final_comparison_results.csv: ì›ì‹œ ë°ì´í„°")  
    print("   - final_drl_comparison.png: ì‹œê°í™” ê²°ê³¼")

if __name__ == "__main__":
    main()