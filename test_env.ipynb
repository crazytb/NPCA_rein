{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPCA Environment 테스트 시작\n",
      "1. 환경 생성 테스트\n",
      "Action space: Discrete(2)\n",
      "Observation space: Dict('can_finish_before_obss': Discrete(2), 'current_backoff': Box(0.0, 1.0, (1,), float32), 'cw_index': Box(0.0, 1.0, (1,), float32), 'npca_intra_busy': Discrete(2), 'npca_obss_remain': Box(0.0, 1.0, (1,), float32), 'ppdu_duration': Box(0.0, 1.0, (1,), float32), 'primary_intra_busy': Discrete(2), 'primary_obss_remain': Box(0.0, 1.0, (1,), float32), 'recent_success_rate': Box(0.0, 1.0, (1,), float32))\n",
      "2. Reset 테스트\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Reset 테스트\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial observation shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial observation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInfo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# test_npca_env.ipynb\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# drl_framework 경로 추가\n",
    "sys.path.append('drl_framework')\n",
    "from NPCAEnv import NPCAEnv, make_npca_env\n",
    "\n",
    "print(\"NPCA Environment 테스트 시작\")\n",
    "\n",
    "# %%\n",
    "# 기본 환경 생성 테스트\n",
    "print(\"1. 환경 생성 테스트\")\n",
    "\n",
    "env = NPCAEnv(\n",
    "    max_ppdu_duration=50,\n",
    "    max_obss_duration=100,\n",
    "    max_backoff=1024,\n",
    "    max_cw_index=6,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# %%\n",
    "# Reset 테스트\n",
    "print(\"2. Reset 테스트\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Initial observation: {obs}\")\n",
    "print(f\"Info: {info}\")\n",
    "\n",
    "# 관찰값 해석 (Dict 형태)\n",
    "print(\"\\n관찰값 해석:\")\n",
    "print(f\"Primary OBSS remain: {obs['primary_obss_remain'][0]:.3f} (실제: {int(obs['primary_obss_remain'][0] * env.max_obss_duration)})\")\n",
    "print(f\"Primary intra busy: {obs['primary_intra_busy']}\")\n",
    "print(f\"NPCA intra busy: {obs['npca_intra_busy']}\")\n",
    "print(f\"NPCA OBSS remain: {obs['npca_obss_remain'][0]:.3f} (실제: {int(obs['npca_obss_remain'][0] * env.max_obss_duration)})\")\n",
    "print(f\"Current backoff: {obs['current_backoff'][0]:.3f} (실제: {int(obs['current_backoff'][0] * env.max_backoff)})\")\n",
    "print(f\"CW index: {obs['cw_index'][0]:.3f} (실제: {int(obs['cw_index'][0] * env.max_cw_index)})\")\n",
    "print(f\"Success rate: {obs['recent_success_rate'][0]:.3f}\")\n",
    "print(f\"PPDU duration: {obs['ppdu_duration'][0]:.3f} (실제: {int(obs['ppdu_duration'][0] * env.max_ppdu_duration)})\")\n",
    "print(f\"Can finish before OBSS: {obs['can_finish_before_obss']}\")\n",
    "\n",
    "# %%\n",
    "# Step 테스트 - 여러 행동 시도\n",
    "print(\"3. Step 테스트\")\n",
    "\n",
    "actions = [0, 1, 0, 1, 1]  # PRIMARY 대기, NPCA 사용 등\n",
    "rewards = []\n",
    "observations = []\n",
    "\n",
    "for i, action in enumerate(actions):\n",
    "    print(f\"\\n--- Step {i+1}: Action {action} ({'PRIMARY 대기' if action == 0 else 'NPCA 사용'}) ---\")\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    observations.append(obs.copy())\n",
    "    \n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(f\"Done: {done}, Truncated: {truncated}\")\n",
    "    print(f\"PPDU duration: {int(obs['ppdu_duration'][0] * env.max_ppdu_duration)}\")\n",
    "    print(f\"Primary OBSS remain: {int(obs['primary_obss_remain'][0] * env.max_obss_duration)}\")\n",
    "    print(f\"Can finish before OBSS: {bool(obs['can_finish_before_obss'])}\")\n",
    "    \n",
    "    if done or truncated:\n",
    "        print(\"Episode ended\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n총 보상: {sum(rewards)}\")\n",
    "\n",
    "# %%\n",
    "# 랜덤 에이전트 테스트 (더 긴 에피소드)\n",
    "print(\"4. 랜덤 에이전트 테스트\")\n",
    "\n",
    "np.random.seed(42)\n",
    "env.reset(seed=42)\n",
    "\n",
    "episode_rewards = []\n",
    "episode_actions = []\n",
    "episode_ppdu_durations = []\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "for step in range(50):  # 50스텝 실행\n",
    "    # 랜덤 행동 선택\n",
    "    action = np.random.choice([0, 1])\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    episode_rewards.append(reward)\n",
    "    episode_actions.append(action)\n",
    "    episode_ppdu_durations.append(int(obs['ppdu_duration'][0] * env.max_ppdu_duration))\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"에피소드 완료: {steps} 스텝\")\n",
    "print(f\"총 보상: {total_reward}\")\n",
    "print(f\"평균 보상: {total_reward/steps:.2f}\")\n",
    "\n",
    "# 행동 분석\n",
    "action_counts = {0: episode_actions.count(0), 1: episode_actions.count(1)}\n",
    "print(f\"행동 분포: PRIMARY 대기 {action_counts[0]}회, NPCA 사용 {action_counts[1]}회\")\n",
    "\n",
    "# %%\n",
    "# 시각화\n",
    "print(\"5. 결과 시각화\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 보상 시계열\n",
    "axes[0,0].plot(episode_rewards)\n",
    "axes[0,0].set_title('Episode Rewards')\n",
    "axes[0,0].set_xlabel('Step')\n",
    "axes[0,0].set_ylabel('Reward')\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# 행동 시계열\n",
    "axes[0,1].plot(episode_actions, 'o-', alpha=0.7)\n",
    "axes[0,1].set_title('Actions (0=PRIMARY, 1=NPCA)')\n",
    "axes[0,1].set_xlabel('Step')\n",
    "axes[0,1].set_ylabel('Action')\n",
    "axes[0,1].set_ylim(-0.1, 1.1)\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# PPDU duration 분포\n",
    "axes[1,0].hist(episode_ppdu_durations, bins=10, alpha=0.7)\n",
    "axes[1,0].set_title('PPDU Duration Distribution')\n",
    "axes[1,0].set_xlabel('PPDU Duration')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# 행동별 평균 보상\n",
    "action_rewards = {0: [], 1: []}\n",
    "for action, reward in zip(episode_actions, episode_rewards):\n",
    "    action_rewards[action].append(reward)\n",
    "\n",
    "avg_rewards = [np.mean(action_rewards[0]) if action_rewards[0] else 0,\n",
    "               np.mean(action_rewards[1]) if action_rewards[1] else 0]\n",
    "\n",
    "axes[1,1].bar(['PRIMARY', 'NPCA'], avg_rewards, alpha=0.7)\n",
    "axes[1,1].set_title('Average Reward by Action')\n",
    "axes[1,1].set_ylabel('Average Reward')\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# 환경 파라미터별 테스트\n",
    "print(\"6. 환경 파라미터 영향 테스트\")\n",
    "\n",
    "# 다양한 OBSS generation rate로 테스트\n",
    "obss_rates = [0.05, 0.1, 0.2, 0.3]\n",
    "results = {}\n",
    "\n",
    "for rate in obss_rates:\n",
    "    print(f\"\\nOBSS generation rate: {rate}\")\n",
    "    \n",
    "    # 환경 재생성 (실제로는 동적으로 바꿀 수 없으므로 시뮬레이션)\n",
    "    env_test = NPCAEnv(max_obss_duration=100, seed=42)\n",
    "    \n",
    "    # 간단한 테스트 실행\n",
    "    env_test.reset()\n",
    "    test_rewards = []\n",
    "    \n",
    "    for _ in range(20):\n",
    "        # OBSS 상황을 인위적으로 만들기\n",
    "        env_test.primary_obss_remain = max(1, int(np.random.exponential(1/rate) * 10))\n",
    "        \n",
    "        action = np.random.choice([0, 1])\n",
    "        obs, reward, done, truncated, info = env_test.step(action)\n",
    "        test_rewards.append(reward)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    results[rate] = {\n",
    "        'avg_reward': np.mean(test_rewards),\n",
    "        'total_reward': np.sum(test_rewards),\n",
    "        'steps': len(test_rewards)\n",
    "    }\n",
    "    \n",
    "    print(f\"  평균 보상: {results[rate]['avg_reward']:.2f}\")\n",
    "    print(f\"  총 보상: {results[rate]['total_reward']:.2f}\")\n",
    "\n",
    "# %%\n",
    "print(\"7. 환경 정상성 검증\")\n",
    "\n",
    "# 상태 공간 범위 확인\n",
    "print(\"상태 공간 검증:\")\n",
    "for _ in range(10):\n",
    "    obs, _ = env.reset()\n",
    "    # Dict 형태의 관찰값 검증\n",
    "    for key, value in obs.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            assert np.all(value >= 0.0) and np.all(value <= 1.0), f\"{key}가 [0,1] 범위를 벗어남: {value}\"\n",
    "        elif key in ['primary_intra_busy', 'npca_intra_busy', 'can_finish_before_obss']:\n",
    "            assert value in [0, 1], f\"{key}가 0 또는 1이 아님: {value}\"\n",
    "\n",
    "print(\"✓ 모든 관찰값이 올바른 범위 내에 있음\")\n",
    "\n",
    "# 행동 공간 확인\n",
    "print(\"\\n행동 공간 검증:\")\n",
    "valid_actions = [0, 1]\n",
    "for action in valid_actions:\n",
    "    try:\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        print(f\"✓ Action {action} 정상 동작\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Action {action} 오류: {e}\")\n",
    "\n",
    "# 보상 범위 확인\n",
    "print(\"\\n보상 범위 검증:\")\n",
    "env.reset()\n",
    "rewards_sample = []\n",
    "for _ in range(100):\n",
    "    action = np.random.choice([0, 1])\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    rewards_sample.append(reward)\n",
    "\n",
    "print(f\"보상 범위: {min(rewards_sample):.2f} ~ {max(rewards_sample):.2f}\")\n",
    "print(f\"평균 보상: {np.mean(rewards_sample):.2f}\")\n",
    "\n",
    "print(\"\\n🎉 NPCAEnv 기본 동작 테스트 완료!\")\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
