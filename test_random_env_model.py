#!/usr/bin/env python3
"""
Random Environment Model ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” random environmentë¡œ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ 
ê¸°ì¡´ fixed environment ëª¨ë¸ë“¤ê³¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import torch
import numpy as np
from pathlib import Path
from npca_semi_mdp_env import NPCASemiMDPEnv
from drl_framework.network import DQN
import time

def dict_to_legacy_vector(obs_dict):
    """Semi-MDP Dict ê´€ì°°ì„ ê¸°ì¡´ 4ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜"""
    return [
        float(obs_dict.get('obss_remaining', 0)),
        float(obs_dict.get('current_slot', 1)),
        33.0,  # tx_duration (ê³ ì •ê°’)
        float(obs_dict.get('cw_index', 0))
    ]

def load_drl_model(model_path, device='cpu'):
    """DRL ëª¨ë¸ ë¡œë“œ ë° ì •ì±… í•¨ìˆ˜ ìƒì„±"""
    try:
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        # ëª¨ë¸ ê´€ì°° ê³µê°„ í¬ê¸° í™•ì¸
        state_dict = checkpoint['policy_net_state_dict']
        input_size = state_dict['layer1.weight'].shape[1]
        print(f"  ëª¨ë¸ ì…ë ¥ í¬ê¸°: {input_size} ì°¨ì›")
        
        model = DQN(n_observations=input_size, n_actions=2).to(device)
        model.load_state_dict(checkpoint['policy_net_state_dict'])
        model.eval()
        
        if input_size == 4:
            # 4ì°¨ì› ë²¡í„° ëª¨ë¸ (ê¸°ì¡´ ëª¨ë¸)
            def policy_func(obs_dict):
                obs_vector = dict_to_legacy_vector(obs_dict)
                input_tensor = torch.tensor(obs_vector, dtype=torch.float32, device=device).unsqueeze(0)
                with torch.no_grad():
                    q_values = model(input_tensor)
                    action = q_values.argmax(dim=1).item()
                return action
        else:
            # ì „ì²´ ê´€ì°° ê³µê°„ ëª¨ë¸ (random environment ëª¨ë¸)
            def policy_func(obs_dict):
                # NPCASemiMDPEnvì˜ flatten_observationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë³€í™˜
                obs_vector = []
                
                # ê¸°ë³¸ ê´€ì°°ê°’ë“¤
                obs_vector.append(float(obs_dict.get('obss_remaining', 0)))
                obs_vector.append(float(obs_dict.get('current_slot', 1)))
                obs_vector.append(float(obs_dict.get('cw_index', 0)))
                
                # í™˜ê²½ íŒŒë¼ë¯¸í„°ë“¤ (random envì—ì„œ ì¶”ê°€ë¨)
                if 'env_obss_duration' in obs_dict:
                    obs_vector.append(float(obs_dict['env_obss_duration']))
                else:
                    obs_vector.append(100.0)  # ê¸°ë³¸ê°’
                    
                if 'env_ppdu_duration' in obs_dict:
                    obs_vector.append(float(obs_dict['env_ppdu_duration']))
                else:
                    obs_vector.append(33.0)  # ê¸°ë³¸ê°’
                
                # ë‚˜ë¨¸ì§€ ê´€ì°°ê°’ë“¤ì„ 0ìœ¼ë¡œ íŒ¨ë”© (í•„ìš”ì‹œ)
                while len(obs_vector) < input_size:
                    obs_vector.append(0.0)
                
                # í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë©´ ìë¥´ê¸°
                obs_vector = obs_vector[:input_size]
                
                input_tensor = torch.tensor(obs_vector, dtype=torch.float32, device=device).unsqueeze(0)
                with torch.no_grad():
                    q_values = model(input_tensor)
                    action = q_values.argmax(dim=1).item()
                return action
        
        return policy_func, checkpoint
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def test_policy(policy_func, policy_name, test_episodes=50, random_env=False, 
                obss_duration=100, ppdu_duration=33):
    """ì •ì±… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    env = NPCASemiMDPEnv(
        num_stas=2,
        num_slots=3000,
        obss_generation_rate=0.05,
        npca_enabled=True,
        throughput_weight=10.0,
        latency_penalty_weight=0.1,
        random_env=random_env
    )
    
    episode_rewards = []
    episode_throughputs = []
    episode_latencies = []
    action_counts = [0, 0]
    
    for episode in range(test_episodes):
        obs, _ = env.reset()
        episode_reward = 0.0
        episode_throughput = 0.0
        episode_latency = 0.0
        
        done = False
        max_decisions = 100
        episode_decisions = 0
        
        while not done and episode_decisions < max_decisions:
            action = policy_func(obs)
            action_counts[action] += 1
            
            try:
                next_obs, reward, done, truncated, info = env.step(action)
                episode_reward += reward
                episode_throughput += info.get('successful_transmission_slots', 0)
                episode_latency += info.get('duration', 0)
                episode_decisions += 1
                obs = next_obs
                
                if done or truncated:
                    break
            except ValueError as e:
                if "step() called when not at decision point" in str(e):
                    break
                else:
                    raise e
        
        episode_rewards.append(episode_reward)
        episode_throughputs.append(episode_throughput)
        episode_latencies.append(episode_latency)
    
    total_actions = sum(action_counts)
    action_probs = [count/total_actions for count in action_counts] if total_actions > 0 else [0.5, 0.5]
    
    return {
        'policy_name': policy_name,
        'avg_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'avg_throughput': np.mean(episode_throughputs),
        'avg_latency': np.mean(episode_latencies),
        'action_distribution': action_probs,
        'episode_rewards': episode_rewards
    }

def main():
    print("ğŸ”¬ Random Environment Model ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    device = torch.device('cpu')
    
    # Random environment ëª¨ë¸ ë¡œë“œ
    random_model_path = "./obss_comparison_results/random_env_robust_model/model.pth"
    random_policy, random_checkpoint = load_drl_model(random_model_path, device)
    
    if not random_policy:
        print("âŒ Random environment ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… Random environment ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    print(f"   í›ˆë ¨ ìŠ¤í…: {random_checkpoint.get('steps_done', 'unknown')}")
    
    # 1. Random environmentì—ì„œ í…ŒìŠ¤íŠ¸ (í›ˆë ¨ í™˜ê²½ê³¼ ë™ì¼)
    print("\nğŸŒŸ Random Environment í…ŒìŠ¤íŠ¸ (í›ˆë ¨ í™˜ê²½ê³¼ ë™ì¼):")
    random_result = test_policy(
        random_policy, 'DRL-Random-Env', 
        test_episodes=100, random_env=True
    )
    
    print(f"  í‰ê·  ë³´ìƒ: {random_result['avg_reward']:.1f} Â± {random_result['std_reward']:.1f}")
    print(f"  í‰ê·  ì²˜ë¦¬ëŸ‰: {random_result['avg_throughput']:.1f}")
    print(f"  í‰ê·  ì§€ì—°: {random_result['avg_latency']:.1f}")
    print(f"  ì•¡ì…˜ ë¶„í¬: Stay {random_result['action_distribution'][0]:.2f}, Switch {random_result['action_distribution'][1]:.2f}")
    
    # 2. ë‹¤ì–‘í•œ ê³ ì • í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ (ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸)
    print("\nğŸ” Fixed Environment í…ŒìŠ¤íŠ¸ (ì¼ë°˜í™” ì„±ëŠ¥):")
    fixed_scenarios = [
        {'obss': 50, 'ppdu': 33, 'name': 'Short OBSS (50)'},
        {'obss': 100, 'ppdu': 33, 'name': 'Medium OBSS (100)'},
        {'obss': 200, 'ppdu': 33, 'name': 'Long OBSS (200)'},
        {'obss': 150, 'ppdu': 33, 'name': 'Custom OBSS (150)'}
    ]
    
    fixed_results = []
    for scenario in fixed_scenarios:
        result = test_policy(
            random_policy, f'DRL-Random-Fixed-{scenario["obss"]}',
            test_episodes=50, random_env=False,
            obss_duration=scenario['obss'], ppdu_duration=scenario['ppdu']
        )
        fixed_results.append(result)
        print(f"  {scenario['name']:18s}: Reward={result['avg_reward']:6.1f}, "
              f"Throughput={result['avg_throughput']:5.1f}, "
              f"Stay/Switch={result['action_distribution'][0]:.2f}/{result['action_distribution'][1]:.2f}")
    
    # 3. ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ë¹„êµ
    print("\nğŸ“Š ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ:")
    fixed_model_paths = [
        ("./obss_comparison_results/ppdu_medium_obss_50/model.pth", "DRL-Fixed-50"),
        ("./obss_comparison_results/ppdu_medium_obss_100/model.pth", "DRL-Fixed-100"),
        ("./obss_comparison_results/ppdu_medium_obss_200/model.pth", "DRL-Fixed-200")
    ]
    
    comparison_results = []
    for model_path, model_name in fixed_model_paths:
        if Path(model_path).exists():
            fixed_policy, _ = load_drl_model(model_path, device)
            if fixed_policy:
                # Random environmentì—ì„œ í…ŒìŠ¤íŠ¸
                result = test_policy(
                    fixed_policy, model_name,
                    test_episodes=50, random_env=True
                )
                comparison_results.append(result)
                print(f"  {model_name:15s}: Reward={result['avg_reward']:6.1f}, "
                      f"Stay/Switch={result['action_distribution'][0]:.2f}/{result['action_distribution'][1]:.2f}")
    
    # 4. ì¢…í•© ë¶„ì„
    print(f"\nğŸ“ˆ ì¢…í•© ì„±ëŠ¥ ë¶„ì„:")
    print("="*50)
    
    # Random environment í‰ê·  ì„±ëŠ¥
    avg_fixed = sum(r['avg_reward'] for r in fixed_results) / len(fixed_results)
    std_fixed = (sum((r['avg_reward'] - avg_fixed)**2 for r in fixed_results) / len(fixed_results))**0.5
    
    print(f"Random Environment ëª¨ë¸:")
    print(f"  Random Env (í›ˆë ¨ í™˜ê²½):  {random_result['avg_reward']:6.1f} Â± {random_result['std_reward']:4.1f}")
    print(f"  Fixed Env (ì¼ë°˜í™”):     {avg_fixed:6.1f} Â± {std_fixed:4.1f}")
    
    if comparison_results:
        avg_comparison = sum(r['avg_reward'] for r in comparison_results) / len(comparison_results)
        print(f"\nFixed Environment ëª¨ë¸ë“¤ (Random Envì—ì„œ):")
        print(f"  í‰ê·  ì„±ëŠ¥:              {avg_comparison:6.1f}")
        
        print(f"\nğŸ’¡ Random Environment ëª¨ë¸ì˜ ì¥ì :")
        if random_result['avg_reward'] > avg_comparison:
            print("  âœ… Random envì—ì„œ ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥")
        if avg_fixed > avg_comparison * 0.9:
            print("  âœ… ë‹¤ì–‘í•œ fixed environmentì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥")
        if random_result['std_reward'] < avg_fixed * 1.2:
            print("  âœ… ì¼ê´€ëœ ì„±ëŠ¥ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ì„±")
    
    print(f"\nğŸ¯ ê²°ë¡ :")
    print(f"Random environment í›ˆë ¨ì€ ë‹¤ì–‘í•œ OBSS/PPDU ì¡°ê±´ì— ê°•ê±´í•œ ì •ì±…ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!")
    
if __name__ == "__main__":
    main()