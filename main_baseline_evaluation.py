#!/usr/bin/env python3
"""
Baseline Evaluation Script for NPCA STA Policies

ì •í™•í•œ baseline ë¹„êµë¥¼ ìœ„í•´ main_semi_mdp_training.pyì™€ ë™ì¼í•œ í™˜ê²½ì—ì„œ
Primary-Only, NPCA-Only ì •ì±…ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from drl_framework.random_access import Channel, STA, Simulator
from drl_framework.configs import (
    PPDU_DURATION,
    PPDU_DURATION_VARIANTS,
    RADIO_TRANSITION_TIME,
    OBSS_GENERATION_RATE,
    DEFAULT_NUM_STAS_CH0,
    DEFAULT_NUM_STAS_CH1,
)

def create_baseline_config(obss_duration=None, ppdu_variant='medium', random_ppdu=False):
    """trainingê³¼ ë™ì¼í•œ ì„¤ì • ìƒì„±"""
    obss_duration = obss_duration or 100
    ppdu_duration = PPDU_DURATION_VARIANTS.get(ppdu_variant, PPDU_DURATION)

    channels = [
        Channel(channel_id=0, obss_generation_rate=OBSS_GENERATION_RATE['secondary']),
        Channel(channel_id=1, obss_generation_rate=OBSS_GENERATION_RATE['primary'],
                obss_duration_range=(obss_duration, obss_duration))
    ]

    stas_config = []

    # CH0 STAë“¤ (NPCA ë¹„í™œì„±í™”)
    for i in range(DEFAULT_NUM_STAS_CH0):
        stas_config.append({
            "sta_id": i,
            "channel_id": 0,
            "npca_enabled": False,
            "ppdu_duration": ppdu_duration,
            "radio_transition_time": RADIO_TRANSITION_TIME
        })

    # CH1 STAë“¤ (NPCA í™œì„±í™”)
    for i in range(DEFAULT_NUM_STAS_CH1):
        stas_config.append({
            "sta_id": i,
            "channel_id": 1,
            "npca_enabled": True,
            "ppdu_duration": ppdu_duration,
            "radio_transition_time": RADIO_TRANSITION_TIME
        })

    return channels, stas_config

class FixedPolicy:
    """ê³ ì • ì •ì±… í´ë˜ìŠ¤"""

    @staticmethod
    def primary_only():
        """í•­ìƒ Primary ì±„ë„ì— ë¨¸ë¬´ë¦„"""
        return 0

    @staticmethod
    def npca_only():
        """í•­ìƒ NPCA ì±„ë„ë¡œ ì´ë™"""
        return 1

    @staticmethod
    def random():
        """ëœë¤í•˜ê²Œ ì„ íƒ"""
        return np.random.choice([0, 1])

def evaluate_baseline_policy(policy_func, policy_name, channels, stas_config,
                           num_episodes=100, num_slots_per_episode=11111, random_ppdu=False):
    """baseline ì •ì±… í‰ê°€ (trainingê³¼ ë™ì¼í•œ í™˜ê²½)"""

    print(f"Evaluating {policy_name}...")

    episode_rewards = []
    action_counts = [0, 0]  # [Stay, Switch]
    decision_counts = []

    for episode in range(num_episodes):
        # ì±„ë„ ìƒíƒœ ì´ˆê¸°í™”
        for ch in channels:
            ch.intra_occupied = False
            ch.intra_end_slot = 0
            ch.obss_traffic = []
            ch.occupied_remain = 0
            ch.obss_remain = 0

        # STA ìƒì„± (trainingê³¼ ë™ì¼)
        stas = []
        for config in stas_config:
            sta = STA(
                sta_id=config["sta_id"],
                channel_id=config["channel_id"],
                primary_channel=channels[config["channel_id"]],
                npca_channel=channels[0] if config["channel_id"] == 1 else None,
                npca_enabled=config.get("npca_enabled", False),
                radio_transition_time=config.get("radio_transition_time", 1),
                ppdu_duration=config.get("ppdu_duration", 33),
                random_ppdu=random_ppdu,
                learner=None,
                num_slots_per_episode=num_slots_per_episode
            )

            # NPCA ì§€ì› STAì— ê³ ì • ì •ì±… ì ìš©
            if config.get("npca_enabled", False):
                sta._fixed_action = policy_func
                sta.decision_count = 0  # ê²°ì • íšŸìˆ˜ ì¶”ì 
                sta.action_history = []  # ì•¡ì…˜ ê¸°ë¡ ì¶”ì  (Random ì •ì±…ìš©)

            stas.append(sta)

        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        simulator = Simulator(num_slots=num_slots_per_episode, channels=channels, stas=stas)
        simulator.memory = None  # baseline í‰ê°€ì—ì„œëŠ” memory ë¶ˆí•„ìš”
        simulator.device = torch.device("cpu")
        simulator.run()

        # ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ ìˆ˜ì§‘ (trainingê³¼ ë™ì¼í•œ ë°©ì‹)
        total_reward = 0
        total_decisions = 0
        episode_actions = [0, 0]

        for sta in stas:
            if sta.npca_enabled:
                total_reward += sta.new_episode_reward
                decisions = getattr(sta, 'decision_count', 0)
                total_decisions += decisions

                # ì •ì±…ì— ë”°ë¥¸ ì•¡ì…˜ ë¶„í¬
                if policy_name == "Primary-Only":
                    episode_actions[0] += decisions
                elif policy_name == "NPCA-Only":
                    episode_actions[1] += decisions
                elif policy_name == "Random":
                    # Random ì •ì±…ì˜ ê²½ìš° ì‹¤ì œ ì•¡ì…˜ ê¸°ë¡ì„ ì¶”ì í•´ì•¼ í•¨
                    action_history = getattr(sta, 'action_history', [])
                    for action in action_history:
                        episode_actions[action] += 1

            sta.new_episode_reward = 0.0

        episode_rewards.append(total_reward)
        action_counts[0] += episode_actions[0]
        action_counts[1] += episode_actions[1]
        decision_counts.append(total_decisions)

    # í†µê³„ ê³„ì‚°
    avg_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    avg_decisions = np.mean(decision_counts)

    total_actions = sum(action_counts)
    action_probs = [count/max(total_actions, 1) for count in action_counts]

    return {
        'policy_name': policy_name,
        'episode_rewards': episode_rewards,
        'avg_reward': avg_reward,
        'std_reward': std_reward,
        'action_distribution': action_probs,
        'avg_decisions': avg_decisions,
        'total_decisions': total_actions
    }

def run_baseline_comparison(obss_duration=100, ppdu_variant='medium',
                          num_episodes=100, num_slots_per_episode=11111, random_ppdu=False):
    """ëª¨ë“  baseline ì •ì±… ë¹„êµ"""

    ppdu_description = "Random (20-200 slots)" if random_ppdu else f"{ppdu_variant} (fixed)"

    print(f"\n{'='*60}")
    print(f"BASELINE POLICY COMPARISON")
    print(f"OBSS Duration: {obss_duration}, PPDU: {ppdu_description}")
    print(f"Episodes: {num_episodes}, Slots per episode: {num_slots_per_episode}")
    print(f"STA Configuration: CH0={DEFAULT_NUM_STAS_CH0}, CH1={DEFAULT_NUM_STAS_CH1}")
    print(f"{'='*60}")

    # í™˜ê²½ ì„¤ì • (trainingê³¼ ë™ì¼)
    channels, stas_config = create_baseline_config(obss_duration, ppdu_variant, random_ppdu)

    results = []

    # Primary-Only ì •ì±… í‰ê°€
    primary_result = evaluate_baseline_policy(
        FixedPolicy.primary_only, "Primary-Only",
        channels, stas_config, num_episodes, num_slots_per_episode, random_ppdu
    )
    results.append(primary_result)

    # NPCA-Only ì •ì±… í‰ê°€
    npca_result = evaluate_baseline_policy(
        FixedPolicy.npca_only, "NPCA-Only",
        channels, stas_config, num_episodes, num_slots_per_episode, random_ppdu
    )
    results.append(npca_result)

    # Random ì •ì±… í‰ê°€
    random_result = evaluate_baseline_policy(
        FixedPolicy.random, "Random",
        channels, stas_config, num_episodes, num_slots_per_episode, random_ppdu
    )
    results.append(random_result)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nBaseline Results:")
    print(f"{'Policy':<15} {'Avg Reward':<12} {'Std':<8} {'Stay%':<8} {'Switch%':<8}")
    print("-" * 55)

    for result in results:
        stay_pct = result['action_distribution'][0] * 100
        switch_pct = result['action_distribution'][1] * 100
        print(f"{result['policy_name']:<15} {result['avg_reward']:<12.2f} "
              f"{result['std_reward']:<8.2f} {stay_pct:<8.0f} {switch_pct:<8.0f}")

    return results

def evaluate_drl_policy(model_path, channels, stas_config, num_episodes=100, num_slots_per_episode=11111, random_ppdu=False):
    """DRL ì •ì±…ì„ baselineê³¼ ë™ì¼í•œ í™˜ê²½ì—ì„œ í‰ê°€"""

    try:
        # ëª¨ë¸ ë¡œë“œ
        device = torch.device("cpu")
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        from drl_framework.network import DQN
        from drl_framework.train import SemiMDPLearner

        # DQN ëª¨ë¸ ìƒì„± ë° ë¡œë“œ
        policy_net = DQN(n_observations=4, n_actions=2).to(device)
        policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        policy_net.eval()

        # Mock learner ìƒì„± (í‰ê°€ìš©)
        class MockLearner:
            def __init__(self, policy_net, device):
                self.policy_net = policy_net
                self.device = device
                self.memory = None
                self.steps_done = 0

            def select_action(self, state_tensor):
                with torch.no_grad():
                    if state_tensor.dim() == 1:
                        state_tensor = state_tensor.unsqueeze(0)
                    q_values = self.policy_net(state_tensor)
                    return q_values.max(1)[1].item()

        mock_learner = MockLearner(policy_net, device)

        print(f"Evaluating DRL Model...")

        episode_rewards = []
        action_counts = [0, 0]  # [Stay, Switch]
        decision_counts = []

        for episode in range(num_episodes):
            # ì±„ë„ ìƒíƒœ ì´ˆê¸°í™”
            for ch in channels:
                ch.intra_occupied = False
                ch.intra_end_slot = 0
                ch.obss_traffic = []
                ch.occupied_remain = 0
                ch.obss_remain = 0

            # STA ìƒì„± (baselineê³¼ ë™ì¼)
            stas = []
            for config in stas_config:
                sta = STA(
                    sta_id=config["sta_id"],
                    channel_id=config["channel_id"],
                    primary_channel=channels[config["channel_id"]],
                    npca_channel=channels[0] if config["channel_id"] == 1 else None,
                    npca_enabled=config.get("npca_enabled", False),
                    radio_transition_time=config.get("radio_transition_time", 1),
                    ppdu_duration=config.get("ppdu_duration", 33),
                    random_ppdu=random_ppdu,
                    learner=mock_learner if config.get("npca_enabled", False) else None,
                    num_slots_per_episode=num_slots_per_episode
                )

                # ê²°ì • ì¶”ì ì„ ìœ„í•œ ì„¤ì •
                if config.get("npca_enabled", False):
                    sta.decision_count = 0
                    sta.action_history = []  # ì•¡ì…˜ ê¸°ë¡ìš©

                stas.append(sta)

            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
            simulator = Simulator(num_slots=num_slots_per_episode, channels=channels, stas=stas)
            simulator.memory = mock_learner.memory
            simulator.device = device
            simulator.run()

            # ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ ìˆ˜ì§‘
            total_reward = 0
            total_decisions = 0
            episode_actions = [0, 0]

            for sta in stas:
                if sta.npca_enabled:
                    total_reward += sta.new_episode_reward
                    decisions = getattr(sta, 'decision_count', 0)
                    total_decisions += decisions

                    # ì•¡ì…˜ ê¸°ë¡ì—ì„œ ì‹¤ì œ ë¶„í¬ ê³„ì‚° (ì¶”ì •ì´ ì•„ë‹Œ ì‹¤ì œê°’)
                    action_history = getattr(sta, 'action_history', [])
                    for action in action_history:
                        episode_actions[action] += 1

                sta.new_episode_reward = 0.0

            episode_rewards.append(total_reward)
            action_counts[0] += episode_actions[0]
            action_counts[1] += episode_actions[1]
            decision_counts.append(total_decisions)

        # í†µê³„ ê³„ì‚°
        avg_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        avg_decisions = np.mean(decision_counts)

        total_actions = sum(action_counts)
        action_probs = [count/max(total_actions, 1) for count in action_counts]

        return {
            'policy_name': 'DRL',
            'episode_rewards': episode_rewards,
            'avg_reward': avg_reward,
            'std_reward': std_reward,
            'action_distribution': action_probs,
            'avg_decisions': avg_decisions,
            'total_decisions': total_actions
        }

    except Exception as e:
        print(f"Error evaluating DRL model: {e}")
        return None

def compare_with_drl_model(obss_duration=100, ppdu_variant='medium', model_path=None, random_ppdu=True):
    """DRL ëª¨ë¸ê³¼ baseline ë¹„êµ (ë™ì¼í•œ í™˜ê²½ì—ì„œ ì‹¤ì œ í‰ê°€)"""

    if model_path is None:
        # ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œë“¤
        model_paths = [
            f"./density_comparison_results/ch0_{DEFAULT_NUM_STAS_CH0}_ch1_{DEFAULT_NUM_STAS_CH1}/model.pth",
            f"./density_comparison_results/obss_{obss_duration}_slots/model.pth"
        ]

        for path in model_paths:
            if os.path.exists(path):
                model_path = path
                break

    # í™˜ê²½ ì„¤ì • (baselineê³¼ ë™ì¼)
    channels, stas_config = create_baseline_config(obss_duration, ppdu_variant, random_ppdu)

    # Baseline ê²°ê³¼
    baseline_results = run_baseline_comparison(obss_duration, ppdu_variant,
                                             num_episodes=100, num_slots_per_episode=11111,
                                             random_ppdu=random_ppdu)

    # DRL ëª¨ë¸ ê²°ê³¼ (ë™ì¼í•œ í™˜ê²½ì—ì„œ ì‹¤ì œ í‰ê°€)
    drl_result = None
    if model_path and os.path.exists(model_path):
        print(f"\nğŸ¤– Evaluating DRL model: {model_path}")
        drl_result = evaluate_drl_policy(
            model_path, channels, stas_config,
            num_episodes=100, num_slots_per_episode=11111, random_ppdu=True
        )

        if drl_result:
            print(f"DRL: Avg Reward = {drl_result['avg_reward']:.2f} Â± {drl_result['std_reward']:.2f}")
            print(f"  Action Dist: Stay={drl_result['action_distribution'][0]:.2f}, Switch={drl_result['action_distribution'][1]:.2f}")
    else:
        print("âš ï¸ No DRL model found for evaluation")

    # ë¹„êµ ì‹œê°í™”
    all_results = baseline_results.copy()
    if drl_result:
        all_results.append(drl_result)

    create_comparison_plot(all_results, obss_duration)

    return baseline_results, drl_result

def create_comparison_plot(all_results, obss_duration=100):
    """ëª¨ë“  ì •ì±… ê²°ê³¼ ë¹„êµ í”Œë¡¯"""

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # í‰ê·  ë³´ìƒ ë¹„êµ
    ax1 = axes[0]
    policies = [r['policy_name'] for r in all_results]
    rewards = [r['avg_reward'] for r in all_results]
    colors = ['lightcoral', 'lightgreen', 'lightblue', 'gold'][:len(all_results)]

    bars = ax1.bar(policies, rewards, color=colors)
    ax1.set_title(f'Policy Comparison (OBSS Duration: {obss_duration})')
    ax1.set_ylabel('Average Reward')
    ax1.grid(True, alpha=0.3)

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bar, reward in zip(bars, rewards):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(rewards)*0.01,
                f'{reward:.2f}', ha='center', fontweight='bold')

    # ì•¡ì…˜ ë¶„í¬ ë¹„êµ (ëª¨ë“  ì •ì±…)
    ax2 = axes[1]
    policy_names = [r['policy_name'] for r in all_results]
    stay_probs = [r['action_distribution'][0] for r in all_results]
    switch_probs = [r['action_distribution'][1] for r in all_results]

    x_pos = np.arange(len(policy_names))
    width = 0.35

    ax2.bar(x_pos - width/2, stay_probs, width, label='Stay Primary', color='lightblue')
    ax2.bar(x_pos + width/2, switch_probs, width, label='Switch NPCA', color='lightsalmon')

    ax2.set_title('Action Distribution (All Policies)')
    ax2.set_ylabel('Action Probability')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(policy_names)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    # ì €ì¥
    os.makedirs("./baseline_comparison_results", exist_ok=True)

    # ax1 plotì„ ë³„ë„ë¡œ PNGì™€ EPSë¡œ ì €ì¥
    fig1, ax1_single = plt.subplots(figsize=(8, 6))
    bars = ax1_single.bar(policies, rewards, color=colors)
    # ax1_single.set_title(f'Policy Comparison (OBSS Duration: {obss_duration})')
    ax1_single.set_ylabel('Average Reward')
    ax1_single.grid(True, alpha=0.3)

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bar, reward in zip(bars, rewards):
        ax1_single.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(rewards)*0.01,
                       f'{reward:.2f}', ha='center', fontweight='bold')

    # ax1 plot PNG ì €ì¥
    fig1.savefig(f"./baseline_comparison_results/policy_comparison_obss_{obss_duration}.png",
                 dpi=300, bbox_inches='tight')

    # ax1 plot EPS ì €ì¥
    fig1.savefig(f"./baseline_comparison_results/policy_comparison_obss_{obss_duration}.eps",
                 format='eps', bbox_inches='tight')

    plt.close(fig1)

    # ì „ì²´ í”Œë¡¯ ì €ì¥
    plt.savefig(f"./baseline_comparison_results/baseline_comparison_obss_{obss_duration}.png",
                dpi=300, bbox_inches='tight')
    plt.show()

    print(f"\nComparison plot saved to ./baseline_comparison_results/")
    print(f"ax1 plot saved as PNG and EPS: policy_comparison_obss_{obss_duration}.*")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬
    obss_duration = 100
    if len(sys.argv) > 1:
        try:
            obss_duration = int(sys.argv[1])
        except ValueError:
            print("Invalid OBSS duration. Using default (100).")

    print(f"ğŸ” Starting baseline evaluation...")
    print(f"Testing environment: OBSS Duration = {obss_duration} slots")

    # Baselineê³¼ DRL ëª¨ë¸ ë¹„êµ (ëœë¤ PPDU ì‚¬ìš©)
    baseline_results, drl_result = compare_with_drl_model(obss_duration, random_ppdu=True)

    # ìš”ì•½ ì¶œë ¥
    print(f"\n{'ğŸ¯ SUMMARY:'}")
    print("-" * 50)

    best_baseline = max(baseline_results, key=lambda x: x['avg_reward'])
    print(f"Best Baseline: {best_baseline['policy_name']} ({best_baseline['avg_reward']:.2f})")

    if drl_result:
        print(f"DRL Model: {drl_result['avg_reward']:.2f}")
        improvement = drl_result['avg_reward'] - best_baseline['avg_reward']
        if improvement > 0:
            print(f"ğŸ† DRL improves over best baseline by {improvement:.2f} points")
        else:
            print(f"âš ï¸ DRL underperforms best baseline by {-improvement:.2f} points")
    else:
        print("âš ï¸ No DRL model found for comparison")

    print(f"\nâœ… Baseline evaluation complete!")

if __name__ == "__main__":
    main()